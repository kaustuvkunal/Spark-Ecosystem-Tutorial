{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "- A scalable and fault-tolerant stream processing engine \n",
    "- Introduced from Spark 2.3 release onwards\n",
    "- Built on Spark SQL library,  based on Dataframe and Dataset APIs\n",
    "- Can easily apply any SQL query (using DataFrame API) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working \n",
    "\n",
    "- Structured Streaming works on the same architecture of polling the data after some duration, based on your trigger interval\n",
    "- In Structured streaming, there is no concept of a batch\n",
    "\n",
    "- The received data in a trigger is appended to the continuously flowing data stream, Each row of the data stream is processed and the result is updated into the unbounded result table.\n",
    "\n",
    "- Structured Streaming uses Dataframe and Dataset APIs to perform streaming operations.\n",
    "\n",
    "- DataFrames are more optimized in terms of processing and provides more options of aggregations and other operations with a variety of functions available.\n",
    "\n",
    "\n",
    "### Advantages \n",
    "\n",
    "- Compatible with event-time data processing.\n",
    "\n",
    "- Structured streaming provides the functionality to process the data on the basis of event-time when the timestamp of the event is included in the data received and prevent data loss if older data arrives late.\n",
    "\n",
    "- Other than checkpointing, Structured streaming has applied two conditions to recover from any error\n",
    "\n",
    "   1.  The source must be replayable\n",
    "   2. The Sinks must support idempotent operations to support reprocessing in case of failures.\n",
    "   \n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary classes and create a local SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a streaming DataFrame that represents text data received from a server listening on localhost:9999,\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# to run open terminal and enter command \n",
    "#nc -lk 9999\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended\n",
    "- Basically, in this so called  stream processing model, You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the unbounded input table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic concepts \n",
    "- Treat a live data stream as a table that is being continuously appended.\n",
    "- Spark runs it as an incremental query on this unbounded input table.\n",
    "- query on the input will generate the “Result Table”. Every trigger interval \n",
    "\n",
    "- “Output” is defined as what gets written out to the external storage and can be defined in a different mode:\n",
    "    - <B>Complete Mode</B> - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "    \n",
    "    - <b>Append Mode</b> - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "    \n",
    "    - <b>Update Mode </b> - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Note that Structured Streaming does not materialize the entire table. It reads the latest available data from the streaming data source, processes it incrementally to update the result, and then discards the source data. It only keeps around the minimal intermediate state data as required to update the result (e.g. intermediate counts in the earlier example).</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Event-time and Late Data\n",
    "- One Advantage of Spark straming is its  handling of late event data \n",
    "- Spark 2.1, we have support for watermarking which allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state.\n",
    "- It  has full control over updating old aggregates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance Semantics\n",
    "\n",
    "The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.\n",
    "\n",
    "- Computation does not start from scrach \n",
    "- Streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream\n",
    "- engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.\n",
    "- streaming sinks are designed to be  handle reprocessing for multiple identical request\n",
    "- Structured Streaming can ensure end-to-end exactly-once semantics under any failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programing\n",
    "- Use the common entry point SparkSession (Scala/Java/Python/R docs) to create streaming.\n",
    "- Streaming DataFrames can be created through the DataStreamReader interface returned by SparkSession.readStream()\n",
    "- Also, you can specify the details of the source – data format, schema, options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Sources\n",
    "\n",
    "#### File source   \n",
    " - Reads files written in a directory as a stream of data.\n",
    " - Files will be processed in the order of file modification time (iflatestFirst is set, order will be reversed)\n",
    " - Supported file formats are text, CSV, JSON, ORC, Parquet\n",
    "\n",
    "#### Kafka source - \n",
    "   - It’s compatible with Kafka broker versions 0.10.0 or higher\n",
    "\n",
    "#### Socket source (for testing) - \n",
    "   -   Reads UTF8 text data from a socket connection.\n",
    "   - Should be used only for testing as this does not provide end-to-end fault-tolerance guarantees.\n",
    "\n",
    "#### Rate source (for testing) - \n",
    "  - ata at the specified number of rows per second, each output row contains a timestamp (time of message dispatch) and value (message count 0 for row 1 ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparksession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read text from socket\n",
    "socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "socketDF.isStreaming()    # Returns True for DataFrames that have streaming sources\n",
    "\n",
    "socketDF.printSchema()\n",
    "\n",
    "# Read all the csv files written atomically in a directory\n",
    "userSchema = StructType().add(\"name\", \"string\").add(\"age\", \"integer\")\n",
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(\"/path/to/directory\")  # Equivalent to format(\"csv\").load(\"/path/to/directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also create streaming DataFrames from tables with DataStreamReader.table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema inference and partition of streaming DataFrames/Datasets\n",
    "\n",
    "- By deafult schema is should to be specified\n",
    "- The directories that make up the partitioning scheme must be present when the query starts and must remain static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  SQL-like operations and yped RDD-like operations (e.g. map, filter, flatMap) both can be applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the devices which have signal more than 10\n",
    "df.select(\"device\").where(\"signal > 10\")\n",
    "\n",
    "# Running count of the number of updates for each device type\n",
    "df.groupBy(\"deviceType\").count()\n",
    "\n",
    "# register a streaming DataFrame/Dataset as a temporary view and then apply SQL commands \n",
    "df.createOrReplaceTempView(\"updates\")\n",
    "spark.sql(\"select count(*) from updates\")  # returns another streaming DF\n",
    "\n",
    "# identify whether a DataFrame/Dataset has streaming data or not by using df.isStreaming.\n",
    "df.isStreaming()\n",
    "\n",
    "# Window Operations on Event Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programing advises\n",
    "- check the query plan of the query,\n",
    "- if Once stateful operations are injected in the query plan, you may need to check your query with considerations in stateful operations. (e.g. output mode, watermark, state store size maintenance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Operations on Event Time\n",
    "- Since this windowing is similar to grouping, in code, you can use groupBy() and window() operations to express windowed aggregations. You can see the full code for the below examples in Scala/Java/Python.\n",
    "- Use `window` as In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "    words.word\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Late Data and Watermarking\n",
    "\n",
    "- Structured Streaming can maintain the intermediate state for partial aggregates for a long period of time such that late data can update aggregates of old windows correctly\n",
    "\n",
    "- However, system has to bound the amount of intermediate in-memory state it accumulates.\n",
    "\n",
    "- System needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more.\n",
    "\n",
    "- To enable this, in Spark 2.1, we have introduced watermarking, which lets the engine automatically track the current event time in the data and attempt to clean up old state accordingly.\n",
    "- , late data within the threshold will be aggregated, but data later than the threshold will start getting dropped (\n",
    "\n",
    "- define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time. For a specific window ending at time T, the engine will maintain state and allow late data to update the state until (max event time seen by the engine - late threshold > T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "        words.word) \\\n",
    "    .count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following conditions must be satisfied for the watermarking to clean the state in aggregation queries\n",
    "\n",
    "- Output mode must be Append or Update.\n",
    "\n",
    "- The aggregation must have either the event-time column, or a window on the event-time column.\n",
    "\n",
    "- withWatermark() must be called on the same column as the timestamp column used in the aggregate\n",
    "\n",
    "- withWatermark must be called before the aggregation for the watermark details to be used.\n",
    "\n",
    "- A watermark delay (set with withWatermark) of “2 hours” guarantees that the engine will never drop any data that is less than 2 hours delayed. Data delayed by more than 2 hours is not guaranteed to be dropped; it may or may not get aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Operations\n",
    "- Structured Streaming supports joining a streaming Dataset/DataFrame with a static Dataset/DataFrame as well as another streaming Dataset/DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDf = spark.read. ...\n",
    "streamingDf = spark.readStream. ...\n",
    "streamingDf.join(staticDf, \"type\")  # inner equi-join with a static DF\n",
    "streamingDf.join(staticDf, \"type\", \"left_outer\")  # left outer join with a static DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream-stream Joins :\n",
    "    for both the input streams, we buffer past input as streaming state, so that we can match every future input with past input and accordingly generate joined results.\n",
    "    \n",
    "##### types of supported stream-stream joins\n",
    "\n",
    " - Inner Joins with optional Watermarking\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "impressions = spark.readStream. ...\n",
    "clicks = spark.readStream. ...\n",
    "\n",
    "# Apply watermarks on event-time columns\n",
    "impressionsWithWatermark = impressions.withWatermark(\"impressionTime\", \"2 hours\")\n",
    "clicksWithWatermark = clicks.withWatermark(\"clickTime\", \"3 hours\")\n",
    "\n",
    "# Join with event-time constraints\n",
    "impressionsWithWatermark.join(\n",
    "  clicksWithWatermark,\n",
    "  expr(\"\"\"\n",
    "    clickAdId = impressionAdId AND\n",
    "    clickTime >= impressionTime AND\n",
    "    clickTime <= impressionTime + interval 1 hour\n",
    "    \"\"\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer Joins with Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "impressionsWithWatermark.join(\n",
    "  clicksWithWatermark,\n",
    "  expr(\"\"\"\n",
    "    clickAdId = impressionAdId AND\n",
    "    clickTime >= impressionTime AND\n",
    "    clickTime <= impressionTime + interval 1 hour\n",
    "    \"\"\"),\n",
    "  \"leftOuter\"                 # can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi Joins with Watermarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Deduplication (eliminate duplicate or redundant information)\n",
    "- can deduplicate records in data streams using a unique identifier in the events\n",
    "- can use deduplication with or without watermarking.\n",
    "    - With watermark : If there is an upper bound on how late a duplicate record may arrive, then you can define a watermark on an event time column and deduplicate using both the guid and the event time columns. query will use the watermark to remove old state data from past records that are not expected to get any duplicates any more\n",
    "    - Without watermark  :Since there are no bounds on when a duplicate record may arrive, the query stores the data from all the past records as state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "streamingDf = spark.readStream. ...\n",
    "\n",
    "# Without watermark using guid column\n",
    "streamingDf.dropDuplicates(\"guid\")\n",
    "\n",
    "# With watermark using guid and eventTime columns\n",
    "streamingDf \\\n",
    "  .withWatermark(\"eventTime\", \"10 seconds\") \\\n",
    "  .dropDuplicates(\"guid\", \"eventTime\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy for handling multiple watermarks\n",
    "- treaming query can have multiple input streams that are unioned or joined together,Each can have a different threshold of late data that needs to be tolerated for stateful operations.You specify these thresholds using withWatermarks (\"eventTime\", delay) on each of the input streams.\n",
    "\n",
    "- If you are to track sessions from data streams of events,you will have to save arbitrary types of data as state, and perform arbitrary operations on the state using the data stream events in every trigger. Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupported Operations\n",
    "There are a few DataFrame/Dataset operations that are not supported with streaming DataFrames/Datasets. Some of them are as follows.\n",
    "\n",
    "- Multiple streaming aggregations\n",
    "- Limit and take the first N rows are not supported on streaming Datasets.\n",
    "- Distinct operations on streaming Datasets are not supported.\n",
    "- Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\n",
    "\n",
    "\n",
    "Following Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset.u will see an AnalysisException like “operation XYZ is not supported with streaming \n",
    "\n",
    "   -  count() - Cannot return a single count from a streaming Dataset. Instead, use ds.groupBy().count() which returns a streaming Dataset containing a running count.\n",
    "\n",
    "   - foreach() - Instead use ds.writeStream.foreach(...) (see next section).\n",
    "\n",
    "   - show() - Instead use the console sink (see next section).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of global watermark\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========== DF with no aggregations ==========\n",
    "noAggDF = deviceDataDf.select(\"device\").where(\"signal > 10\")   \n",
    "\n",
    "# Print new data to console\n",
    "noAggDF \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Write new data to Parquet files\n",
    "noAggDF \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .option(\"path\", \"path/to/destination/dir\") \\\n",
    "    .start()\n",
    "\n",
    "# ========== DF with aggregation ==========\n",
    "aggDF = df.groupBy(\"device\").count()\n",
    "\n",
    "# Print updated aggregations to console\n",
    "aggDF \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Have all the aggregates in an in-memory table. The query name will be the table name\n",
    "aggDF \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"aggregates\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "spark.sql(\"select * from aggregates\").show()   # interactively query in-memory table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Table APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = ...  # spark session\n",
    "\n",
    "# Create a streaming DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n",
    "\n",
    "# Write the streaming DataFrame to a table\n",
    "df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .toTable(\"myTable\")\n",
    "\n",
    "# Check the table result\n",
    "spark.read.table(\"myTable\").show()\n",
    "\n",
    "# Transform the source dataset and write to a new table\n",
    "spark.readStream \\\n",
    "    .table(\"myTable\") \\\n",
    "    .select(\"value\") \\\n",
    "    .writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .toTable(\"newTable\")\n",
    "\n",
    "# Check the new table result\n",
    "spark.read.table(\"newTable\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "- trigger settings of a streaming query define the timing of streaming data processing,\n",
    "- whether the query is going to be executed as micro-batch query with a fixed batch interval or as a continuous processing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Default trigger (runs micro-batch as soon as it can)\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .start()\n",
    "\n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(processingTime='2 seconds') \\\n",
    "  .start()\n",
    "\n",
    "# One-time trigger\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .start()\n",
    "\n",
    "# Continuous trigger with one-second checkpointing interval\n",
    "df.writeStream\n",
    "  .format(\"console\")\n",
    "  .trigger(continuous='1 second')\n",
    "  .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Streaming Queries\n",
    "- The StreamingQuery object created when a query is started can be used to monitor and manage the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = df.writeStream.format(\"console\").start()   # get the query object\n",
    "\n",
    "query.id()          # get the unique identifier of the running query that persists across restarts from checkpoint data\n",
    "\n",
    "query.runId()       # get the unique id of this run of the query, which will be generated at every start/restart\n",
    "\n",
    "query.name()        # get the name of the auto-generated or user-specified name\n",
    "\n",
    "query.explain()   # print detailed explanations of the query\n",
    "\n",
    "query.stop()      # stop the query\n",
    "\n",
    "query.awaitTermination()   # block until query is terminated, with stop() or with error\n",
    "\n",
    "query.exception()       # the exception if the query has been terminated with error\n",
    "\n",
    "query.recentProgress()  # an array of the most recent progress updates for this query\n",
    "\n",
    "query.lastProgress()    # the most recent progress update of this streaming query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start any number of queries in a single SparkSession. They will all be running concurrently sharing the cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = ...  # spark session\n",
    "\n",
    "spark.streams.active  # get the list of currently active streaming queries\n",
    "\n",
    "spark.streams.get(id)  # get a query object by its unique id\n",
    "\n",
    "spark.streams.awaitAnyTermination()  # block until any one of them terminates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Streaming Queries\n",
    "- There are multiple ways to monitor active streaming queries. You can either push metrics to external systems using Spark’s Dropwizard Metrics support, or access them programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reading Metrics Interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = ...  # a StreamingQuery\n",
    "print(query.lastProgress)\n",
    "\n",
    "'''\n",
    "Will print something like the following.\n",
    "\n",
    "{u'stateOperators': [], u'eventTime': {u'watermark': u'2016-12-14T18:45:24.873Z'}, u'name': u'MyQuery', u'timestamp': u'2016-12-14T18:45:24.873Z', u'processedRowsPerSecond': 200.0, u'inputRowsPerSecond': 120.0, u'numInputRows': 10, u'sources': [{u'description': u'KafkaSource[Subscribe[topic-0]]', u'endOffset': {u'topic-0': {u'1': 134, u'0': 534, u'3': 21, u'2': 0, u'4': 115}}, u'processedRowsPerSecond': 200.0, u'inputRowsPerSecond': 120.0, u'numInputRows': 10, u'startOffset': {u'topic-0': {u'1': 1, u'0': 1, u'3': 1, u'2': 0, u'4': 1}}}], u'durationMs': {u'getOffset': 2, u'triggerExecution': 3}, u'runId': u'88e2ff94-ede0-45a8-b687-6316fbef529a', u'id': u'ce011fdc-8762-4dcb-84eb-a77333e28109', u'sink': {u'description': u'MemorySink'}}\n",
    "'''\n",
    "\n",
    "print(query.status)\n",
    "''' \n",
    "Will print something like the following.\n",
    "\n",
    "{u'message': u'Waiting for data to arrive', u'isTriggerActive': False, u'isDataAvailable': False}\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reporting Metrics programmatically\n",
    "\n",
    "- You can also asynchronously monitor all queries associated with a SparkSession by attaching a StreamingQueryListener\n",
    "\n",
    "- Not available in Python.\n",
    "\n",
    "#### Reporting Metrics using Dropwizard\n",
    "- Spark supports reporting metrics using the Dropwizard Library. To enable metrics of Structured Streaming queries to be reported as well, you have to explicitly enable the configuration spark.sql.streaming.metricsEnabled in the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "# or\n",
    "spark.sql(\"SET spark.sql.streaming.metricsEnabled=true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering from Failures with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aggDF \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovery Semantics after Changes in a Streaming Query\n",
    "Here are a few kinds of changes that are either not allowed, or the effect of the change is not well-defined\n",
    "\n",
    "- Changes in the number or type (i.e. different source) of input sources:\n",
    "- Changes in the parameters of input sources: \n",
    "- Changes in the type of output sink: \n",
    "- Changes in the parameters of output sink:\n",
    "- Changes in projection / filter / map-like operations\n",
    "- Changes in stateful operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Processing\n",
    "\n",
    "- Continuous processing is a new, experimental streaming execution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees.\n",
    "- Remember,default micro-batch processing engine which can achieve exactly-once guarantees but achieve latencies of ~100ms at best\n",
    "- To run a supported query in continuous processing mode, all you need to do is specify a continuous trigger with the desired checkpoint interval as a parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load() \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .trigger(continuous=\"1 second\") \\     # only change in query\n",
    "  .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Queries\n",
    "\n",
    "only the following type of queries are supported in the continuous processing mode.\n",
    "    \n",
    "   - Operations: \n",
    "       - Only map-like Dataset/DataFrame operations are supported in continuous mode, that is\n",
    "       - only projections (select, map, flatMap, mapPartitions, etc.)\n",
    "       - and selections (where, filter, etc.)\n",
    "    \n",
    "   - Sources:\n",
    "       - Kafka source: All options are supported.\n",
    "       - Rate source: Good for testing. Only options that are supported in the continuous mode are numPartitions and rowsPerSecond.\n",
    "       \n",
    "   - Sinks: \n",
    "       - Kafka sink: All options are supported.\n",
    "       - Memory sink: Good for debugging.\n",
    "       - Console sink: Good for debugging. All options are supported. Note that the console will print every checkpoint interval that you have specified in the continuous trigger.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats (usage warnings)\n",
    "\n",
    "- Before starting a continuous processing query, you must ensure there are enough cores in the cluster to all the tasks in parallel.\n",
    "\n",
    "- There are currently no automatic retries of failed tasks\n",
    "\n",
    "- Some usefull configurations to know \n",
    "    - spark.sql.shuffle.partitions\n",
    "    - spark.sql.streaming.stateStore.providerClass\n",
    "    - spark.sql.streaming.multipleWatermarkPolicy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
