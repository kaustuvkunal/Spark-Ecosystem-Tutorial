{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Basic Statistics\n",
    "\n",
    "Spark version : 2.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics (DF)\n",
    "\n",
    "- mllib provides statistics library \n",
    "- colStats available in Statistics : provide column summary statistics for RDD[Vector]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.  20. 200.]\n",
      "[1.e+00 1.e+02 1.e+04]\n",
      "[3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "mat = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])]\n",
    ")  # an RDD of Vectors\n",
    "\n",
    "# Compute column summary statistics.\n",
    "summary = Statistics.colStats(mat)\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation (DF)\n",
    "- Calculate the correlation between two series of data \n",
    "- Supported correlation methods are currently Pearson’s and Spearman’s correlation\n",
    "- Correlation computes the correlation matrix for the input Dataset of Vectors using the specified method\n",
    "- Output will be a DataFrame that contains the correlation matrix of the column of vectors\n",
    "- Depending on the type of input, two RDD[Double]s or an RDD[Vector], the output will be a Double or the correlation Matrix respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation is: 0.8500286768773001\n",
      "[[1.         0.97888347 0.99038957]\n",
      " [0.97888347 1.         0.99774832]\n",
      " [0.99038957 0.99774832 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n",
    "# seriesY must have the same number of partitions and cardinality as seriesX\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n",
    "\n",
    "# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(\"Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))\n",
    "\n",
    "data = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]\n",
    ")  # an RDD of Vectors\n",
    "\n",
    "# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(Statistics.corr(data, method=\"pearson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an RDD of any key value pairs\n",
    "data = sc.parallelize([(1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')])\n",
    "\n",
    "# specify the exact fraction desired from each key as a dictionary\n",
    "fractions = {1: 0.1, 2: 0.6, 3: 0.3}\n",
    "\n",
    "approxSample = data.sampleByKey(False, fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing (DF)\n",
    "- Hypothesis testing determine whether a result is statistically significant, whether this result occurred by chance or not.\n",
    "- spark.ml currently supports Pearson’s Chi-squared ( χ2) tests for independence.\n",
    "\n",
    "#### ChiSquareTest \n",
    "- ChiSquareTest conducts Pearson’s independence test for every feature against the label\n",
    "-  For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed.\n",
    "- All label and feature values must be categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|  features|\n",
      "+-----+----------+\n",
      "|  0.0|[0.5,10.0]|\n",
      "|  0.0|[1.5,20.0]|\n",
      "|  1.0|[1.5,30.0]|\n",
      "|  0.0|[3.5,30.0]|\n",
      "|  0.0|[3.5,40.0]|\n",
      "|  1.0|[3.5,40.0]|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 4 \n",
      "statistic = 0.12499999999999999 \n",
      "pValue = 0.998126379239318 \n",
      "No presumption against null hypothesis: observed follows the same distribution as expected..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 2 \n",
      "statistic = 0.14141414141414144 \n",
      "pValue = 0.931734784568187 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Column 1:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 2:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 3:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "vec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)  # a vector composed of the frequencies of events\n",
    "\n",
    "# compute the goodness of fit. If a second vector to test against\n",
    "# is not supplied as a parameter, the test runs against a uniform distribution.\n",
    "goodnessOfFitTestResult = Statistics.chiSqTest(vec)\n",
    "\n",
    "# summary of the test including the p-value, degrees of freedom,\n",
    "# test statistic, the method used, and the null hypothesis.\n",
    "print(\"%s\\n\" % goodnessOfFitTestResult)\n",
    "\n",
    "mat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0])  # a contingency matrix\n",
    "\n",
    "# conduct Pearson's independence test on the input contingency matrix\n",
    "independenceTestResult = Statistics.chiSqTest(mat)\n",
    "\n",
    "# summary of the test including the p-value, degrees of freedom,\n",
    "# test statistic, the method used, and the null hypothesis.\n",
    "print(\"%s\\n\" % independenceTestResult)\n",
    "\n",
    "obs = sc.parallelize(\n",
    "    [LabeledPoint(1.0, [1.0, 0.0, 3.0]),\n",
    "     LabeledPoint(1.0, [1.0, 2.0, 0.0]),\n",
    "     LabeledPoint(1.0, [-1.0, 0.0, -0.5])]\n",
    ")  # LabeledPoint(label, feature)\n",
    "\n",
    "# The contingency table is constructed from an RDD of LabeledPoint and used to conduct\n",
    "# the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n",
    "# against the label.\n",
    "featureTestResults = Statistics.chiSqTest(obs)\n",
    "\n",
    "for i, result in enumerate(featureTestResults):\n",
    "    print(\"Column %d:\\n%s\" % (i + 1, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.539827837277029 \n",
      "pValue = 0.06821463111921133 \n",
      "Low presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "parallelData = sc.parallelize([0.1, 0.15, 0.2, 0.3, 0.25])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "# summary of the test including the p-value, test statistic, and null hypothesis\n",
    "# if our p-value indicates significance, we can reject the null hypothesis\n",
    "# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n",
    "# a lambda to calculate the CDF is not made available in the Python API\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random data generation (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n",
    "# standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\n",
    "u = RandomRDDs.normalRDD(sc, 1000000 , 10)\n",
    "# Apply a transform to get a random double RDD following `N(1, 4)`.\n",
    "v = u.map(lambda x: 1.0 + 2.0 * x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation (RDD)\n",
    "\n",
    "Kernel density estimation is a technique useful for visualizing empirical probability distributions without requiring assumptions about the particular distribution that the observed samples are drawn from. It computes an estimate of the probability density function of a random variables, evaluated at a given set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "# an RDD of sample data\n",
    "data = sc.parallelize([1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0])\n",
    "\n",
    "# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n",
    "# kernels\n",
    "kd = KernelDensity()\n",
    "kd.setSample(data)\n",
    "kd.setBandwidth(3.0)\n",
    "\n",
    "# Find density estimates for the given values\n",
    "densities = kd.estimate([-1.0, 2.0, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summarizer (DF)\n",
    " - provide vector column summary statistics for Dataframe through Summarizer.\n",
    " - Available metrics are the column-wise max, min, mean, sum, variance, std, and number of nonzeros, as well as the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|[[1.0,1.0,1.0], 1]                 |\n",
      "+-----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|[[1.0,1.5,2.0], 2]              |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\n",
    "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "\n",
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
