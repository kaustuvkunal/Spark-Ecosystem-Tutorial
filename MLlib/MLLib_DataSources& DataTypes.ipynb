{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib : DF \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilizations\n",
    "spark_home = \"file:///Users/kaustuv/spark-2.4.7/\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data sources\n",
    "\n",
    "- MLLib supprts ingestion of various generic data source types including Parquet, CSV, JSON and JDBC. \n",
    "- Specic data source support includes Libsv format files and  image files. For image, the loaded DataFrame has one StructType column: “image”, containing image data stored as image schema. The schema of the image column is\n",
    "- The schema of the image column is:\n",
    "    - origin: StringType (represents the file path of the image)\n",
    "    - height: IntegerType (height of the image)\n",
    "    - width: IntegerType (width of the image)\n",
    "    - nChannels: IntegerType (number of image channels)\n",
    "    - mode: IntegerType (OpenCV-compatible type)\n",
    "    - data: BinaryType (Image bytes in OpenCV-compatible order: row-wise BGR in most cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|              origin|width|height|\n",
      "+--------------------+-----+------+\n",
      "|file:///Users/kau...|  300|   311|\n",
      "|file:///Users/kau...|  199|   313|\n",
      "|file:///Users/kau...|  300|   200|\n",
      "|file:///Users/kau...|  300|   296|\n",
      "+--------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(spark_home +\"data/mllib/images/origin/kittens\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(10,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBSVM data source\n",
    "\n",
    "-  LIBSVM data source is used to load ‘libsvm’ type files from a directory\n",
    "- loaded DataFrame has two columns: label containing labels stored as doubles and features containing feature vectors stored as Vectors\n",
    "- The schemas of the columns are:\n",
    "\n",
    "        label: DoubleType (represents the instance label)\n",
    "        features: VectorUDT (represents the feature vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "|  1.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[151,152,153...|\n",
      "|  0.0|(780,[129,130,131...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[99,100,101,...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[127,128,129...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "df.show(10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML -RDD-based API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types  \n",
    "-  Supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local vector\n",
    "\n",
    "A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Use a NumPy array as a dense vector.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "# Use a Python list as a dense vector.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
    "sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled point\n",
    "Labeled point is a local vector, either dense or sparse, associated with a label/response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse data\n",
    " - MLlib supports reading training examples stored in LIBSVM format, which is the default format used by LIBSVM and LIBLINEAR. It is a text format in which each line represents a labeled sparse feature vector using the following format:\n",
    " - label index1:value1 index2:value2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLUtils.loadLibSVMFile reads training examples stored in LIBSVM format.\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "examples = MLUtils.loadLibSVMFile(sc, spark_home +\"data/mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local matrix\n",
    "\n",
    "- Dense matrix : stored in a one-dimensional array ,with the matrix size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 3, 5, 2, 4, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RowMatrix\n",
    "\n",
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices, backed by an RDD of its rows, where each row is a local vector. Since each row is represented by a local vector, the number of columns is limited by the integer range but it should be much smaller in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndexedRowMatrix\n",
    "\n",
    "An IndexedRowMatrix is similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "# Create an RDD of indexed rows.\n",
    "#   - This can be done explicitly with the IndexedRow class:\n",
    "indexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n",
    "                              IndexedRow(1, [4, 5, 6]),\n",
    "                              IndexedRow(2, [7, 8, 9]),\n",
    "                              IndexedRow(3, [10, 11, 12])])\n",
    "#   - or by using (long, vector) tuples:\n",
    "indexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]),\n",
    "                              (2, [7, 8, 9]), (3, [10, 11, 12])])\n",
    "\n",
    "# Create an IndexedRowMatrix from an RDD of IndexedRows.\n",
    "mat = IndexedRowMatrix(indexedRows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of IndexedRows.\n",
    "rowsRDD = mat.rows\n",
    "\n",
    "# Convert to a RowMatrix by dropping the row indices.\n",
    "rowMat = mat.toRowMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoordinateMatrix\n",
    "\n",
    "A CoordinateMatrix is a distributed matrix backed by an RDD of its entries. Each entry is a tuple of (i: Long, j: Long, value: Double), where i is the row index, j is the column index, and value is the entry value. A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "# Create an RDD of coordinate entries.\n",
    "#   - This can be done explicitly with the MatrixEntry class:\n",
    "entries = sc.parallelize([MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(2, 1, 3.7)])\n",
    "#   - or using (long, long, float) tuples:\n",
    "entries = sc.parallelize([(0, 0, 1.2), (1, 0, 2.1), (2, 1, 3.7)])\n",
    "\n",
    "# Create a CoordinateMatrix from an RDD of MatrixEntries.\n",
    "mat = CoordinateMatrix(entries)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 3\n",
    "n = mat.numCols()  # 2\n",
    "\n",
    "# Get the entries as an RDD of MatrixEntries.\n",
    "entriesRDD = mat.entries\n",
    "\n",
    "# Convert to a RowMatrix.\n",
    "rowMat = mat.toRowMatrix()\n",
    "\n",
    "# Convert to an IndexedRowMatrix.\n",
    "indexedRowMat = mat.toIndexedRowMatrix()\n",
    "\n",
    "# Convert to a BlockMatrix.\n",
    "blockMat = mat.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlockMatrix\n",
    "\n",
    "A BlockMatrix is a distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock. BlockMatrix supports methods such as add and multiply with another BlockMatrix. BlockMatrix also has a helper function validate which can be used to check whether the BlockMatrix is set up properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "mat = BlockMatrix(blocks, 3, 2)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 6\n",
    "n = mat.numCols()  # 2\n",
    "\n",
    "# Get the blocks as an RDD of sub-matrix blocks.\n",
    "blocksRDD = mat.blocks\n",
    "\n",
    "# Convert to a LocalMatrix.\n",
    "localMat = mat.toLocalMatrix()\n",
    "\n",
    "# Convert to an IndexedRowMatrix.\n",
    "indexedRowMat = mat.toIndexedRowMatrix()\n",
    "\n",
    "# Convert to a CoordinateMatrix.\n",
    "coordinateMat = mat.toCoordinateMatrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
