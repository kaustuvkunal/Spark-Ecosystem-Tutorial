{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intilializations\n",
    "spark_version = \"2.4.7\"\n",
    "spark_home = \"file:///Users/kaustuv/spark-2.4.7/\"\n",
    "spark_outfolder = \"file:///Users/kaustuv/Documents/Courses/Spark/modelout/\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "- Clustering algorithms that clusters the data points into a predefined number of clusters\n",
    "- MLlib implementation includes a parallelized variant of the k-means++ method called kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "# for DF API \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> K Means : RDD API</B> <br>\n",
    "In the following example after loading and parsing data, we use the KMeans object to cluster the data into two clusters. The number of desired clusters is passed to the algorithm. We then compute Within Set Sum of Squared Error (WSSSE). You can reduce this error measure by increasing k. In fact the optimal k is usually one where there is an “elbow” in the WSSSE graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(spark_home + \"file:///Users/kaustuv/spark-3.1.1/data/mllib/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"file:///Users/kaustuv/Documents/Courses/Spark/modelout/KMeansModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Latent Dirichlet allocation (LDA)\n",
    " \n",
    " - Implemented as an Estimator  that generates a LDAModel as the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -780.766696363605\n",
      "The upper bound on perplexity: 3.0029488321677116\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[9, 7, 3]  |[0.1098239065792031, 0.09816055275801179, 0.09771350482284155] |\n",
      "|1    |[8, 10, 9] |[0.11038608188739116, 0.10512054014527725, 0.09976959179450495]|\n",
      "|2    |[5, 4, 0]  |[0.1612974535771344, 0.14632933298698012, 0.14596355786974397] |\n",
      "|3    |[3, 9, 10] |[0.29100982941222525, 0.12431457341381975, 0.1126043709146982] |\n",
      "|4    |[6, 10, 9] |[0.21142376712301392, 0.18091236002470296, 0.130266170189281]  |\n",
      "|5    |[4, 1, 7]  |[0.15003575357542334, 0.14341479452923464, 0.11896759355263571]|\n",
      "|6    |[3, 8, 9]  |[0.09868772347307755, 0.0980497534865028, 0.09474718748037855] |\n",
      "|7    |[8, 7, 0]  |[0.1053884100022045, 0.10190010494106766, 0.0964030079004354]  |\n",
      "|8    |[6, 1, 0]  |[0.10156928300856972, 0.10015630171014965, 0.09887422846865833]|\n",
      "|9    |[0, 8, 9]  |[0.11605666092708632, 0.11372522440293081, 0.09621808912920607]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.0046720274625224765,0.0046720350315767965,0.5240302686176949,0.004912014181563718,0.4382678302975383,0.004757798152759471,0.0046720879324899794,0.004671968523873139,0.004671995064034782,0.004671974735946557]    |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007798244654175831,0.0077982593106238635,0.007947683200364705,0.008198910698164997,0.008957012611313996,0.9281066636975729,0.007798301622492662,0.007798297388170439,0.007798358974880841,0.007798267842239736]    |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004061224680067691,0.004061231561392977,0.004139017294553869,0.004269773756447982,0.9630882852036065,0.004135611438283781,0.004061217119423377,0.004061190769307339,0.004061239814632662,0.004061208362283831]     |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003591760621548607,0.0035917644041964336,0.003660296252003293,0.003776605490378882,0.9673549316072338,0.0036575989752303806,0.0035917657242511445,0.0035917584168539424,0.003591767634682862,0.0035917508736206]   |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003891809876168698,0.0038918468889900287,0.003966726558486885,0.9642475708192062,0.004470863938816833,0.003963761558072738,0.0038918600727092228,0.003891814388616545,0.003891880525207339,0.0038918653737254377]  |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.003591889105657006,0.0035918911566182894,0.7362213596735895,0.23444329128801494,0.004126206084541003,0.0036578207972248465,0.003591887101237966,0.003591863598318653,0.0035918931552380282,0.0035918980395598745]  |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.003735692607024355,0.003735698516956006,0.003806976873719414,0.003928130817566642,0.966046534550175,0.003804180672184021,0.0037357008446698508,0.0037356954226735037,0.003735704784874974,0.003735684910156188]    |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004246281018921913,0.004246318032426286,0.004327795297085625,0.9609917601413094,0.004878084731729261,0.004324536253179037,0.00424632572418102,0.004246273121365127,0.0042463340951865115,0.0042462915846157665]    |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004246282519681692,0.004246242865561327,0.5063684450545481,0.004464934335236264,0.004877569946825249,0.4588113504653674,0.004246290937850324,0.0042462827456238505,0.004246333017586542,0.0042462681117188825]     |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.003219695304819342,0.0032197340928763777,0.003281435423714378,0.0033849604137890714,0.9707362560213704,0.0032790445180354145,0.0032197359411777248,0.003219687753325688,0.0032197230801001963,0.003219727450791581]|\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004061212973555537,0.004061201079278181,0.004138827159791988,0.9626929465799284,0.004665380185124472,0.0041356489321967005,0.004061208355105112,0.004061181476282221,0.004061215105178075,0.004061178153559269]    |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.00467180868904125,0.00467178833077975,0.9569341812801653,0.004911537671896996,0.005365876992899425,0.004757602448480423,0.004671784306511237,0.004671782149034093,0.004671830094043027,0.004671808037148548]       |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DF -API \n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> RDD API  :</b>In the following example, we load word count vectors representing a corpus of documents. We then use LDA to infer three topics from the documents. The number of desired clusters is passed to the algorithm. We then output the topics, represented as probability distributions over words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 7.136567545349074\n",
      " 9.17423739769681\n",
      " 2.2973332755243874\n",
      " 7.74896206977181\n",
      " 3.667920066552916\n",
      " 2.7218615703304714\n",
      " 14.290818208156182\n",
      " 1.7005462721131068\n",
      " 4.786929613293138\n",
      " 10.88227104239743\n",
      " 23.420456894402445\n",
      "Topic 1:\n",
      " 9.334046356900249\n",
      " 6.2571239053539465\n",
      " 1.889322674338553\n",
      " 25.143833504691937\n",
      " 13.074261767726298\n",
      " 7.221478143169692\n",
      " 6.281845744850988\n",
      " 3.543274325089574\n",
      " 1.582847027410369\n",
      " 7.365571669744681\n",
      " 4.559958551562936\n",
      "Topic 2:\n",
      " 9.529386097750677\n",
      " 13.568638696949243\n",
      " 7.813344050137059\n",
      " 7.107204425536252\n",
      " 8.257818165720789\n",
      " 12.056660286499838\n",
      " 10.427336046992828\n",
      " 4.7561794027973185\n",
      " 1.630223359296492\n",
      " 5.75215728785789\n",
      " 5.019584554034619\n"
     ]
    }
   ],
   "source": [
    "# RDD API \n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(spark_home + \"data/mllib/sample_lda_data.txt\")\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "# Index documents with unique IDs\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)\n",
    "\n",
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "      + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))\n",
    "\n",
    "# Save and load model\n",
    "ldaModel.save(sc, spark_outfolder + \"LDAModel\")\n",
    "sameModel = LDAModel.load(sc, spark_outfolder +\"LDAModel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting k-means\n",
    "\n",
    "- Bisecting k-means is a kind of hierarchical clustering using a divisive (or “top-down”) approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "- Bisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "# DF API \n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(spark_home+\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model = bkm.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bisecting K-means Cost = 0.11999999999994547\n"
     ]
    }
   ],
   "source": [
    "# RDD -API \n",
    "from numpy import array\n",
    "\n",
    "from pyspark.mllib.clustering import BisectingKMeans\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(spark_home + \"data/mllib/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "model = BisectingKMeans.train(parsedData, 2, maxIterations=5)\n",
    "\n",
    "# Evaluate clustering\n",
    "cost = model.computeCost(parsedData)\n",
    "print(\"Bisecting K-means Cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM)\n",
    "-  Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability\n",
    "\n",
    "- spark.ml implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model given a set of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                         |cov                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  |\n",
      "|[9.099999999999984,9.099999999999984,9.099999999999984]      |0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DF API \n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# loads data\n",
    "dataset = spark.read.format(\"libsvm\").load(spark_home + \"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(538009335)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. The spark.mllib implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model given a set of samples. The implementation has the following parameters:\n",
    "\n",
    "    k is the number of desired clusters.\n",
    "    convergenceTol is the maximum change in log-likelihood at which we consider convergence achieved.\n",
    "    maxIterations is the maximum number of iterations to perform without reaching convergence.\n",
    "    initialModel is an optional starting point from which to start the EM algorithm. If this parameter is omitted, a random starting point will be constructed from the data.\n",
    "\n",
    "\n",
    "- In the following example after loading and parsing data, we use a GaussianMixture object to cluster the data into two clusters. The number of desired clusters is passed to the algorithm. We then output the parameters of the mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight =  0.4803341198536062 mu =  [0.07222436012684245,0.01667856414580816] sigma =  [[4.78126493 1.87708401]\n",
      " [1.87708401 0.91494249]]\n",
      "weight =  0.5196658801463938 mu =  [-0.10439917402842634,0.04285392575445832] sigma =  [[ 4.90562922 -2.00593205]\n",
      " [-2.00593205  1.01114923]]\n"
     ]
    }
   ],
   "source": [
    "# RDD API \n",
    "from numpy import array\n",
    "\n",
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(spark_home + \"data/mllib/gmm_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.strip().split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "gmm = GaussianMixture.train(parsedData, 2)\n",
    "\n",
    "# Save and load model\n",
    "gmm.save(sc, spark_outfolder + \"GaussianMixtureModel\")\n",
    "sameModel = GaussianMixtureModel\\\n",
    "    .load(sc, spark_outfolder +\"GaussianMixtureModel\")\n",
    "\n",
    "# output parameters of model\n",
    "for i in range(2):\n",
    "    print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "          \"sigma = \", gmm.gaussians[i].sigma.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Iteration Clustering (PIC)\n",
    "\n",
    "-  a scalable graph clustering algorithm developed by Lin and Cohen.\n",
    "- PIC finds a very low-dimensional embedding of a dataset using truncated power iteration on a normalized pair-wise similarity matrix of the data.\n",
    "\n",
    "- spark.ml’s PowerIterationClustering implementation takes the following parameters:\n",
    "    - k: the number of clusters to create\n",
    "    - initMode: param for the initialization algorithm\n",
    "    - maxIter: param for maximum number of iterations\n",
    "    - srcCol: param for the name of the input column for source vertex IDs\n",
    "    - dstCol: name of the input column for destination vertex IDs\n",
    "    - weightCol: Param for weight column name\n",
    "\n",
    "\n",
    "\n",
    "- Power iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in Lin and Cohen, Power Iteration Clustering. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero. spark.mllib’s PIC implementation takes the following (hyper-)parameters:\n",
    "   - k: number of clusters\n",
    "   - maxIterations: maximum number of power iterations\n",
    "   - initializationMode: initialization model. This can be either “random”, which is the default, to use a random vector as vertex properties, or “degree” to use normalized sum similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|cluster|\n",
      "+---+-------+\n",
      "|  4|      1|\n",
      "|  0|      0|\n",
      "|  1|      0|\n",
      "|  2|      0|\n",
      "|  3|      1|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 1, 1.0),\n",
    "    (0, 2, 1.0),\n",
    "    (1, 2, 1.0),\n",
    "    (3, 4, 1.0),\n",
    "    (4, 0, 0.1)\n",
    "], [\"src\", \"dst\", \"weight\"])\n",
    "\n",
    "pic = PowerIterationClustering(k=2, maxIter=20, initMode=\"degree\", weightCol=\"weight\")\n",
    "\n",
    "# Shows the cluster assignment\n",
    "pic.assignClusters(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> RDD API : </b><br>\n",
    "PowerIterationClustering implements the PIC algorithm. It takes an RDD of (srcId: Long, dstId: Long, similarity: Double) tuples representing the affinity matrix. Calling PowerIterationClustering.run returns a PowerIterationClusteringModel, which contains the computed clustering assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD API\n",
    "from pyspark.mllib.clustering import PowerIterationClustering, PowerIterationClusteringModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile( spark_home + \"data/mllib/pic_data.txt\")\n",
    "similarities = data.map(lambda line: tuple([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Cluster the data into two classes using PowerIterationClustering\n",
    "model = PowerIterationClustering.train(similarities, 2, 10)\n",
    "\n",
    "model.assignments().foreach(lambda x: print(str(x.id) + \" -> \" + str(x.cluster)))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, spark_outfolder + \"PICModel\")\n",
    "sameModel = PowerIterationClusteringModel\\\n",
    "    .load(sc, spark_outfolder + \"PICModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming k-means\n",
    "\n",
    "When data arrive in a stream, we may want to estimate clusters dynamically, updating them as new data arrive. spark.mllib provides support for streaming k-means clustering, with parameters to control the decay (or “forgetfulness”) of the estimates. The algorithm uses a generalization of the mini-batch k-means update rule. For each batch of data, we assign all points to their nearest cluster, compute new cluster centers, then update each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "#sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-06-14 20:38:12\n",
      "-------------------------------------------\n",
      "(1.0, 0)\n",
      "(2.0, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-14 20:38:13\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "\n",
    "# we make an input stream of vectors for training,\n",
    "# as well as a stream of vectors for testing\n",
    "def parse(lp):\n",
    "    label = float(lp[lp.find('(') + 1: lp.find(')')])\n",
    "    vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))\n",
    "\n",
    "    return LabeledPoint(label, vec)\n",
    "\n",
    "trainingData = sc.textFile(spark_home+ \"data/mllib/kmeans_data.txt\")\\\n",
    "    .map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "\n",
    "testingData = sc.textFile(spark_home +\"data/mllib/streaming_kmeans_data_test.txt\").map(parse)\n",
    "\n",
    "trainingQueue = [trainingData]\n",
    "testingQueue = [testingData]\n",
    "\n",
    "trainingStream = ssc.queueStream(trainingQueue)\n",
    "testingStream = ssc.queueStream(testingQueue)\n",
    "\n",
    "# We create a model with random clusters and specify the number of clusters to find\n",
    "model = StreamingKMeans(k=2, decayFactor=1.0).setRandomCenters(3, 1.0, 0)\n",
    "\n",
    "# Now register the streams for training and testing and start the job,\n",
    "# printing the predicted cluster assignments on new data points as they arrive.\n",
    "model.trainOn(trainingStream)\n",
    "\n",
    "result = model.predictOnValues(testingStream.map(lambda lp: (lp.label, lp.features)))\n",
    "result.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Singular value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg import Vectors\n",
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# rows = sc.parallelize([\n",
    "#     Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "#     Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "#     Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "# ])\n",
    "\n",
    "# mat = RowMatrix(rows)\n",
    "\n",
    "# # Compute the top 5 singular values and corresponding singular vectors.\n",
    "# svd = mat.computeSVD(5, computeU=True)\n",
    "# U = svd.U       # The U factor is a RowMatrix.\n",
    "# s = svd.s       # The singular values are stored in a local dense vector.\n",
    "# V = svd.V       # The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "\n",
    "- Principal component analysis (PCA) is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate, in turn, has the largest variance possible. The columns of the rotation matrix are called principal components. PCA is used widely in dimensionality reduction.\n",
    "\n",
    "- spark.mllib supports PCA for tall-and-skinny matrices stored in row-oriented format and any Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg import Vectors\n",
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# rows = sc.parallelize([\n",
    "#     Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "#     Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "#     Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "# ])\n",
    "\n",
    "# mat = RowMatrix(rows)\n",
    "# # Compute the top 4 principal components.\n",
    "# # Principal components are stored in a local dense matrix.\n",
    "# pc = mat.computePrincipalComponents(4)\n",
    "\n",
    "# # Project the rows to the linear space spanned by the top 4 principal components.\n",
    "# projected = mat.multiply(pc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
