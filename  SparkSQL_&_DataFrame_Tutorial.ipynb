{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between SparkContext , SparkSession and SqlContext\n",
    "\n",
    "-  <b>SparkContext</b> : Used to be an entry point of spark application Prior to 2.0\n",
    "\n",
    "-  <b>SparkSession</b> : has become an entry point to Spark to work with RDD, DataFrame, and Dataset, Since Spark 2.0  hence, PySpark applications start with initializing SparkSession \n",
    "\n",
    "- <b>SQLContext</b> :  is entry point of SparkSQL which can be received from sparkContext\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark context \n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "# appName parameter is a name for your application to show on the cluster UI. \n",
    "#master is a Spark, Mesos or YARN cluster URL\n",
    "conf = SparkConf().setAppName('appName').setMaster('local[*]')\n",
    "#first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/kaustuv/spark-2.4.7/examples/src/main/resources/people.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting some environment variable  \n",
    "SPARK_HOME = \"file:/Users/kaustuv/spark-2.4.7/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL \n",
    "\n",
    "- Spark module for structured data processing, to execute SQL queries\n",
    "- Spark SQL provides Spark, \n",
    "    - Structure information and \n",
    "    - Computation being performed and uses this to to perform extra optimizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read json to dataframe \n",
    "df = spark.read.json(\"file:/Users/kaustuv/spark-3.1.1/examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a PySpark DataFrame from a list of rows\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2020, 1, 1), e=datetime(2020, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2020, 2, 1), e=datetime(2020, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2020, 3, 1), e=datetime(2020, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2020-01-01|2020-01-01 12:00:00|\n",
      "|  2|3.0|string2|2020-02-01|2020-01-02 12:00:00|\n",
      "|  4|5.0|string3|2020-03-01|2020-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PySpark DataFrame with an explicit schema\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PySpark DataFrame from a pandas DataFrame\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PySpark DataFrame from an RDD consisting of a list of tuples.\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All DataFrames above result same.\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top rows of a DataFrame can be displayed using DataFrame.show()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager mode, meaning that all of the data is loaded into memory before the next step begins execution,\n",
    "You can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration for the eager evaluation \n",
    "\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shwing row verticaly, useful when rows are too long to show horizontally.\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collects the distributed data to the driver side as the local data in Python\n",
    "# throw an out-of-memory error when the dataset is too large to fit in the driver side\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail().\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Spark dataframe to panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas APIs\n",
    "pdr = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         a    b\n",
       "count  3.0  3.0\n",
       "mean   2.0  3.0\n",
       "std    1.0  1.0\n",
       "min    1.0  2.0\n",
       "25%    1.5  2.5\n",
       "50%    2.0  3.0\n",
       "75%    2.5  3.5\n",
       "max    3.0  4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   a       3 non-null      int64         \n",
      " 1   b       3 non-null      float64       \n",
      " 2   c       3 non-null      object        \n",
      " 3   d       3 non-null      object        \n",
      " 4   e       3 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(2)\n",
      "memory usage: 248.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "pdr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark, df are from the previous example\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the specified column\n",
    "df.select(\"a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|  a|(b + 1)|\n",
      "+---+-------+\n",
      "|  1|    3.0|\n",
      "|  2|    4.0|\n",
      "|  3|    5.0|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['a'], df['b'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'a'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark is lazily evaluated i.e. simply selecting a column does not trigger the computation\n",
    "df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.c.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of column-wise operations return Columns.\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|string1|\n",
      "|string2|\n",
      "|string3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.select() takes the Column instances that returns another DataFrame.\n",
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assign new Column instance.\n",
    "df.withColumn('upper_c', upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a subset of rows using DataFrame.filter().\n",
    "\n",
    "df.filter(df.a == 1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select column with condition\n",
    "df.filter(df['a'] > 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "|black|    6.0|   60.0|\n",
      "| blue|    3.0|   30.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping and then applying the avg() function to the resulting groups.\n",
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|sum(v1)|sum(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|     24|    240|\n",
      "|black|      6|     60|\n",
      "| blue|      6|     60|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('color').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source : Getting Data in/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/kaustuv/Documents/Courses/Spark/pyspark_notebooks/data/people.json'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining path\n",
    "storage = \"file://\"\n",
    "absolute_path= \"/Users/kaustuv/Documents/Courses/Spark/pyspark_notebooks/\"\n",
    "relative_path = \"data/\"\n",
    "json_file_name = \"people.json\"\n",
    "\n",
    "file_path = storage+ absolute_path +relative_path\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.load(storage+ absolute_path +relative_path +json_file_name, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually Specifying Options\n",
    "- Can manually specify the data source that will be used along with any extra options t\n",
    "- Data sources are specified by their fully qualified name : i.e. org.apache.spark.sql.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parqueat form\n",
    "df.select(\"name\", \"age\").write.save(storage+ absolute_path +relative_path + \"out/namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"people.csv\"\n",
    "df = spark.read.load(storage+ absolute_path +relative_path +csv_file_name ,\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing dataframe into CSV \n",
    "\n",
    "df.write.csv(file_path+'out/out_csv', header=True)\n",
    "\n",
    "# reading  csv into data frame\n",
    "csv_df = spark.read.csv(file_path+'out/out_csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading json into data frame \n",
    "json_df = spark.read.option(\"multiline\",\"true\").json(file_path+'data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- messageTime: long (nullable = true)\n",
      " |-- parameters: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timestamp: long (nullable = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- roadNumber: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------------------------------------------------------------------+----------+\n",
      "|customerId|messageTime  |parameters                                                                     |roadNumber|\n",
      "+----------+-------------+-------------------------------------------------------------------------------+----------+\n",
      "|ABC       |1597352330263|[[speed, 1597352329000, 0], [pcs, 1597352329000, 0], [notch, 1597352329000, 5]]|3802      |\n",
      "+----------+-------------+-------------------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to show all content\n",
    "json_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------------+----------+\n",
      "|customerId|messageTime  |parameters               |roadNumber|\n",
      "+----------+-------------+-------------------------+----------+\n",
      "|ABC       |1597352330263|[speed, 1597352329000, 0]|3802      |\n",
      "|ABC       |1597352330263|[pcs, 1597352329000, 0]  |3802      |\n",
      "|ABC       |1597352330263|[notch, 1597352329000, 5]|3802      |\n",
      "+----------+-------------+-------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode\n",
    "json_df = json_df.withColumn(\"parameters\", explode(json_df.parameters))\n",
    "json_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- messageTime: long (nullable = true)\n",
      " |-- parameters: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      " |    |-- value: long (nullable = true)\n",
      " |-- roadNumber: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- messageTime: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- roadNumber: long (nullable = true)\n",
      "\n",
      "+----------+-------------+-----+-------------+-----+----------+\n",
      "|customerId|messageTime  |name |timestamp    |value|roadNumber|\n",
      "+----------+-------------+-----+-------------+-----+----------+\n",
      "|ABC       |1597352330263|speed|1597352329000|0    |3802      |\n",
      "|ABC       |1597352330263|pcs  |1597352329000|0    |3802      |\n",
      "|ABC       |1597352330263|notch|1597352329000|5    |3802      |\n",
      "+----------+-------------+-----+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 lvel sturcute dataframe into  single level structure\n",
    "json_df = json_df.select(\"customerId\",\"messageTime\", \n",
    "                         col(\"parameters.name\").alias(\"name\"),\n",
    "                         col(\"parameters.timestamp\").alias(\"timestamp\"),\n",
    "                         col(\"parameters.value\").alias(\"value\"),\"roadNumber\")\n",
    "\n",
    "json_df.printSchema()\n",
    "json_df.show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing  df  as CSV \n",
    "# writing dataframe into CSV \n",
    "json_df.write.mode(\"overwrite\").csv(file_path+'out_csv', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing  df as json \n",
    "json_df.write.json(file_path+'out/out_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write as parquet data \n",
    "df.write.parquet(file_path+'bar.parquet')\n",
    "\n",
    "# read as parqueat\n",
    "spark.read.parquet(file_path+'bar.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra options are also used during write operation. For example, you can control bloom filters and dictionary encodings for ORC data sources. The following ORC example will create bloom filter and use dictionary encoding only for favorite_color.\n",
    "\n",
    "<b> Bloom Filters : </b> Bloom filter is a probabilistic data structure designed to tell you, rapidly and memory-efficiently, whether an element is present in a set.It tells us that the element either definitely is not in the set or may be in the set.\n",
    "\n",
    "<b> Dictionary Encodings : </b> A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.orc(file_path +\"users.orc\")\n",
    "(df.write.format(\"orc\")\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
    "    .option(\"orc.column.encoding.direct\", \"name\")\n",
    "    .save(file_path+\"out/users_with_options.orc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.orc(file_path+'out/zoo.orc')\n",
    "spark.read.orc(file_path+'out/zoo.orc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avro \n",
    "- Since spark-avro module is external, there is no .avro API in DataFrameReader or DataFrameWriter.\n",
    "\n",
    "- Different extension of Avro\n",
    "    - .avsc - A JSON representation of an Avro schema (for a single object). This file is parsed by Avro libraries.\n",
    "    - .avpr - A JSON representation of an Avro protocol (a collection of schemas)\n",
    "    - .avdl - A code-like language that gets translated to .avsc or .avpr using the avro-tools.jar. This file is not used by Avro libraries, as far as I can tell.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SQL on files directly\n",
    "df = spark.sql(\"SELECT * FROM parquet.`file:///Users/kaustuv/spark-3.1.1/examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing, Sorting and Partitioning\n",
    "\n",
    "- For file-based data source, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\").saveAsTable(\"people_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"give_absolute_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is possible to use both partitioning and bucketing for a single table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"file:///Users/kaustuv/spark-3.1.1/examples/src/main/resources/users.parquet\")\n",
    "(df\n",
    "    .write\n",
    "    .partitionBy(\"favorite_color\")\n",
    "    .bucketBy(42, \"name\")\n",
    "    .saveAsTable(\"file:///Users/kaustuv/Documents/Courses/Spark/out/people_partitioned_bucketed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with SQL: Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the DataFrame as a table and run a SQL easily as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  UDFs can be registered and invoked in SQL out of the box:\n",
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  SQL expressions can directly be mixed and used as PySpark columns.\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.selectExpr('add_one(v1)').show()\n",
    "df.select(expr('count(*)') > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Temporary View\n",
    " - Temporary views in Spark SQL are session-scoped and \n",
    " - will disappear if the session, that creates it, terminates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "\n",
    "\n",
    "#### Inferring the Schema Using Reflection\n",
    "- Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"file:///Users/kaustuv/spark-3.1.1/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "# Name: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Specifying the Schema\n",
    "\n",
    "- When a dictionary of kwargs cannot be defined ahead of time\n",
    "- a DataFrame can be created programmatically with three steps.\n",
    "    - Create an RDD of tuples or lists from the original RDD;\n",
    "    - Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
    "    - Apply the schema to the RDD via createDataFrame method provided by SparkSession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"file:///Users/kaustuv/spark-3.1.1/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "results.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tuning\n",
    "\n",
    "1. Caching Data In Memory :\n",
    "    - Spark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(\"tableName\") or dataFrame.cache()\n",
    "    - It will automatically tune compression to minimize memory usage and GC pressure\n",
    "    - To remove the table from memory call spark.catalog.uncacheTable(\"tableName\") .\n",
    "    - Configuration of in-memory caching can be done using the setConf method on SparkSession or by running SET key=value commands\n",
    "    \n",
    "    <br></br>\n",
    "    \n",
    "1. Using Configuration Options\n",
    "    - spark.sql.files.maxPartitionBytes\n",
    "    - spark.sql.files.maxPartitionBytes\n",
    "    - spark.sql.files.minPartitionNum\n",
    "    - spark.sql.broadcastTimeout\n",
    "    - spark.sql.autoBroadcastJoinThreshold\n",
    "    - spark.sql.shuffle.partitions\n",
    "    - spark.sql.sources.parallelPartitionDiscovery.threshold\n",
    "    - spark.sql.sources.parallelPartitionDiscovery.parallelism\n",
    "    \n",
    "    <br></br>\n",
    "    \n",
    "1. Join Strategy Hints for SQL Queries\n",
    "    - BROADCAST, MERGE, SHUFFLE_HASH and SHUFFLE_REPLICATE_NL  are some of the joint strategy\n",
    "    - Note that there is no guarantee that Spark will choose the join strategy specified in the hint since a specific strategy may not support all join types\n",
    "    - sample usage : spark.table(\"src\").join(spark.table(\"records\").hint(\"broadcast\"), \"key\").show() \n",
    "    \n",
    "    <br></br>\n",
    "\n",
    "1. Coalesce Hints for SQL Queries\n",
    "    - Coalesce hints allows the Spark SQL users to control the number of output files hence can be used for performance tuning and reducing the number of output files.\n",
    "    - The “COALESCE” hint only has a partition number as a parameter\n",
    "    - The “REPARTITION” hint has a partition number, columns, or both of them as parameters.\n",
    "    - The “REPARTITION_BY_RANGE” hint must have column names and a partition number is optional.\n",
    "    - usage\n",
    "        ```SELECT /*+ COALESCE(3) */ * FROM t\n",
    "        SELECT /*+ REPARTITION(3) */ * FROM t\n",
    "        SELECT /*+ REPARTITION(c) */ * FROM t\n",
    "        SELECT /*+ REPARTITION(3, c) */ * FROM t\n",
    "        SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t\n",
    "        SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t\n",
    "        ```\n",
    "         <br></br>\n",
    "       \n",
    "1. Adaptive Query Execution\n",
    "    - Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan\n",
    "    - Set spark.sql.adaptive.enabled to control  it on/off.\n",
    "    - There are three major features in AQE, \n",
    "        \n",
    "          1. Coalescing post-shuffle partitions\n",
    "              Spark can pick the proper shuffle partition number at runtime once you set a large enough initial number of shuffle partitions via spark.sql.adaptive.coalescePartitions.initialPartitionNum configuration.\n",
    "              \n",
    "                   \n",
    "          2. Converting sort-merge join to broadcast join \n",
    "                  AQE converts sort-merge join to broadcast hash join when the runtime statistics of any join side is smaller than the broadcast hash join threshold\n",
    "                 \n",
    "                  This is not as efficient as planning a broadcast hash join in the first place, but it’s better than keep doing the sort-merge join\n",
    "          \n",
    "          3. Skew join optimization.       \n",
    "       \n",
    "           This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "### Supported Data Types\n",
    "\n",
    "\n",
    "- Numeric types\n",
    "     - ByteType: Represents 1-byte signed integer numbers. The range of numbers is from -128 to 127.\n",
    "     - ShortType: Represents 2-byte signed integer numbers. The range of numbers is from -32768 to 32767.\n",
    "     - IntegerType: Represents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647.\n",
    "     - LongType: Represents 8-byte signed integer numbers. The range of numbers is from - 9223372036854775808 to   9223372036854775807.\n",
    "     \n",
    "     - FloatType: Represents 4-byte single-precision floating point numbers.\n",
    "     - DoubleType: Represents 8-byte double-precision floating point numbers.\n",
    "     - DecimalType: Represents arbitrary-precision signed decimal numbers. Backed internally by java.math.BigDecimal. A BigDecimal consists of an arbitrary precision integer unscaled value and a 32-bit integer scale.\n",
    "    \n",
    "- String type\n",
    "     - StringType: Represents character string values.\n",
    "     - VarcharType(length): A variant of StringType which has a length limitation. Data writing will fail if the input string exceeds the length limitation. Note: this type can only be used in table schema, not functions/operators.\n",
    "     - CharType(length): A variant of VarcharType(length) which is fixed length. Reading column of type CharType(n) always returns string values of length n. Char type column comparison will pad the short one to the longer length.\n",
    "\n",
    "- Binary type\n",
    "    - BinaryType: Represents byte sequence values.\n",
    "    \n",
    "- Boolean type\n",
    "    - BooleanType: Represents boolean values.\n",
    "    \n",
    "- Datetime type\n",
    "    - TimestampType: Represents values comprising values of fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time.\n",
    "    - DateType: Represents values comprising values of fields year, month and day, without a time-zone.\n",
    "    \n",
    "    \n",
    "- Complex types\n",
    "    - ArrayType(elementType, containsNull): Represents values comprising a sequence of elements with the type of elementType. containsNull is used to indicate if elements in a ArrayType value can have null values.\n",
    "    - MapType(keyType, valueType, valueContainsNull): Represents values comprising a set of key-value pairs. The data type of keys is described by keyType and the data type of values is described by valueType. For a MapType value, keys are not allowed to have null values. valueContainsNull is used to indicate if values of a MapType value can have null values.\n",
    "        \n",
    "    - StructType(fields): Represents values with the structure described by a sequence of StructFields (fields).\n",
    "        - StructField(name, dataType, nullable): Represents a field in a StructType. The name of a field is indicated by name. The data type of a field is indicated by dataType. nullable is used to indicate if values of these fields can have null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data types of Spark SQL are located in the package of pyspark.sql.types. You can access them by doing\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN Semantics\n",
    "\n",
    "There is special handling for not-a-number (NaN) when dealing with float or double types that do not exactly match standard floating point semantics. Specifically:\n",
    "\n",
    "-  NaN = NaN returns true.\n",
    "- In aggregations, all NaN values are grouped together.\n",
    "- NaN is treated as a normal value in join keys.\n",
    "- NaN values go last when in ascending order, larger than any other numeric value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SELECT double('infinity') AS col;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SELECT float('-inf') AS col;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark -Sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| col|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT double('infinity') AS col\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| col|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT float('-inf') AS col\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT CAST('a' AS INT);\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT float('NaN') AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT double('infinity') * 0 AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT double('-infinity') * (-1234567) AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT double('infinity') < double('NaN') AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT double('NaN') = double('NaN') AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT double('inf') = double('infinity') AS col;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE test (c1 int, c2 double);\")\n",
    "          \n",
    "          \n",
    "          #INSERT INTO test VALUES (1, double('infinity'));INSERT INTO test VALUES (2, double('infinity'));INSERT INTO test VALUES (3, double('inf'));INSERT INTO test VALUES (4, double('-inf'));INSERT INTO test VALUES (5, double('NaN'));INSERT INTO test VALUES (6, double('NaN'));INSERT INTO test VALUES (7, double('-infinity'));\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between hive  using spark engine and  Spark-SQL\n",
    "\n",
    "- Query  on hive  using spark engine uses hive libraries and its  function  where as  Spark-SQL uses  Spek-sql \n",
    "- Spark-sql can be run without  metastore also just by using dataframe\n",
    "- jobs can be monotor in spark console where hive query are monito in application master console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Functions\n",
    "\n",
    "-  size: This function returns null for null input.\n",
    "    \n",
    "- element_at:\n",
    "    - This function throws ArrayIndexOutOfBoundsException if using invalid indices.\n",
    "    - This function throws NoSuchElementException if key does not exist in map.\n",
    "- elt: This function throws ArrayIndexOutOfBoundsException if using invalid indices.\n",
    "    \n",
    "- parse_url: This function throws IllegalArgumentException if an input string is not a valid url.\n",
    "    \n",
    "- to_date: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.\n",
    "    \n",
    "- to_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.\n",
    "    \n",
    "- unix_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.\n",
    "    \n",
    "- to_unix_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.\n",
    "    \n",
    "- make_date: This function should fail with an exception if the result date is invalid.\n",
    "    \n",
    "- make_timestamp: This function should fail with an exception if the result timestamp is invalid.\n",
    "    \n",
    "- make_interval: This function should fail with an exception if the result interval is invalid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Operators\n",
    "\n",
    "\n",
    "- array_col[index]: This operator throws ArrayIndexOutOfBoundsException if using invalid indices.\n",
    "    \n",
    "- map_col[key]: This operator throws NoSuchElementException if key does not exist in map.\n",
    "    \n",
    "- CAST(string_col AS TIMESTAMP): This operator should fail with an exception if the input string can’t be parsed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Keywords\n",
    "\n",
    "- Reserved keywords: Keywords that are reserved and can’t be used as identifiers for table, view, column, function, alias, etc.\n",
    "- Non-reserved keywords: Keywords that have a special meaning only in particular contexts and can be used as identifiers in other contexts. For example, EXPLAIN SELECT ... is a command, but EXPLAIN can be used as identifiers in other places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Functions\n",
    "\n",
    "#### Aggregate Functions\n",
    "\n",
    "Function \tDescription\n",
    "\n",
    "- any(expr): \tReturns true if at least one value of `expr` is true.\n",
    "\n",
    "- approx_count_distinct(expr[, relativeSD]): \tReturns the estimated cardinality by HyperLogLog++. `relativeSD` defines the maximum relative standard deviation allowed.\n",
    "\n",
    "- approx_percentile(col, percentage [, accuracy]) :\tReturns the approximate `percentile` of the numeric column `col` which is the smallest value in the ordered `col` values (sorted from least to greatest) such that no more than `percentage` of `col` values is less than the value or equal to that value. The value of percentage must be between 0.0 and 1.0. The `accuracy` parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is the relative error of the approximation. When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column `col` at the given percentage array.\n",
    "\n",
    "- avg(expr): \tReturns the mean calculated from values of a group.\n",
    "\n",
    "- bit_and(expr): \tReturns the bitwise AND of all non-null input values, or null if none.\n",
    "- bit_or(expr): \tReturns the bitwise OR of all non-null input values, or null if none.\n",
    "- bit_xor(expr): \tReturns the bitwise XOR of all non-null input values, or null if none.\n",
    "- bool_and(expr): \tReturns true if all values of `expr` are true.\n",
    "- bool_or(expr): \tReturns true if at least one value of `expr` is true.\n",
    "- collect_list(expr): \tCollects and returns a list of non-unique elements.\n",
    "- collect_set(expr): \tCollects and returns a set of unique elements.\n",
    "- corr(expr1, expr2): \tReturns Pearson coefficient of correlation between a set of number pairs.\n",
    "- count(*): \tReturns the total number of retrieved rows, including rows containing null.\n",
    "- count(expr[, expr...]):\tReturns the number of rows for which the supplied expression(s) are all non-null.\n",
    "- count(DISTINCT expr[, expr...]): \tReturns the number of rows for which the supplied expression(s) are unique and non-null.\n",
    "- count_if(expr): \tReturns the number of `TRUE` values for the expression.\n",
    "- count_min_sketch(col, eps, confidence, seed): \tReturns a count-min sketch of a column with the given esp, confidence and seed. The result is an array of bytes, which can be deserialized to a `CountMinSketch` before usage. Count-min sketch is a probabilistic data structure used for cardinality estimation using sub-linear space.\n",
    "\n",
    "- covar_pop(expr1, expr2): \tReturns the population covariance of a set of number pairs.\n",
    "\n",
    "- covar_samp(expr1, expr2): \tReturns the sample covariance of a set of number pairs.\n",
    "\n",
    "- every(expr): \tReturns true if all values of `expr` are true.\n",
    "\n",
    "- first(expr[, isIgnoreNull]): \tReturns the first value of `expr` for a group of rows. If `isIgnoreNull` is true, returns only non-null values.\n",
    "- first_value(expr[, isIgnoreNull]) :\tReturns the first value of `expr` for a group of rows. If `isIgnoreNull` is true, returns only non-null values.\n",
    "\n",
    "- kurtosis(expr): \tReturns the kurtosis value calculated from values of a group.\n",
    "\n",
    "- last(expr[, isIgnoreNull]): \tReturns the last value of `expr` for a group of rows. If `isIgnoreNull` is true, returns only non-null values\n",
    "\n",
    "- last_value(expr[, isIgnoreNull]): \tReturns the last value of `expr` for a group of rows. If `isIgnoreNull` is true, returns only non-null values\n",
    "\n",
    "- max(expr) :\tReturns the maximum value of `expr`.\n",
    "- max_by(x, y): \tReturns the value of `x` associated with the maximum value of `y`.\n",
    "- mean(expr): \tReturns the mean calculated from values of a group.\n",
    "- min(expr): \tReturns the minimum value of `expr`.\n",
    "- min_by(x, y): \tReturns the value of `x` associated with the minimum value of `y`.\n",
    "- percentile(col, percentage [, frequency]): \tReturns the exact percentile value of numeric column `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. The value of frequency should be positive integral\n",
    "- percentile(col, array(percentage1 [, percentage2]...) [, frequency]): \tReturns the exact percentile value array of numeric column `col` at the given percentage(s). Each value of the percentage array must be between 0.0 and 1.0. The value of frequency should be positive integral\n",
    "- percentile_approx(col, percentage [, accuracy]): \tReturns the approximate `percentile` of the numeric column `col` which is the smallest value in the ordered `col` values (sorted from least to greatest) such that no more than `percentage` of `col` values is less than the value or equal to that value. The value of percentage must be between 0.0 and 1.0. The `accuracy` parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is the relative error of the approximation. When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column `col` at the given percentage array.\n",
    "- skewness(expr) :\tReturns the skewness value calculated from values of a group.\n",
    "- some(expr): \tReturns true if at least one value of `expr` is true.\n",
    "- std(expr): \tReturns the sample standard deviation calculated from values of a group.\n",
    "- stddev(expr): \tReturns the sample standard deviation calculated from values of a group.\n",
    "- stddev_pop(expr): \tReturns the population standard deviation calculated from values of a group.\n",
    "- stddev_samp(expr): \tReturns the sample standard deviation calculated from values of a group.\n",
    "- sum(expr): \tReturns the sum calculated from values of a group.\n",
    "- var_pop(expr): \tReturns the population variance calculated from values of a group.\n",
    "- var_samp(expr): \tReturns the sample variance calculated from values of a group.\n",
    "- variance(expr): \tReturns the sample variance calculated from values of a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Functions\n",
    "\n",
    "\n",
    " \n",
    "- cume_dist(): \tComputes the position of a value relative to all values in the partition.\n",
    "\n",
    "- dense_rank(): \tComputes the rank of a value in a group of values. The result is one plus the previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps in the ranking sequence.\n",
    "\n",
    "- lag(input[, offset[, default]]): \tReturns the value of `input` at the `offset`th row before the current row in the window. The default value of `offset` is 1 and the default value of `default` is null. If the value of `input` at the `offset`th row is null, null is returned. If there is no such offset row (e.g., when the offset is 1, the first row of the window does not have any previous row), `default` is returned.\n",
    "\n",
    "- lead(input[, offset[, default]]): \tReturns the value of `input` at the `offset`th row after the current row in the window. The default value of `offset` is 1 and the default value of `default` is null. If the value of `input` at the `offset`th row is null, null is returned. If there is no such an offset row (e.g., when the offset is 1, the last row of the window does not have any subsequent row), `default` is returned.\n",
    "\n",
    "- nth_value(input[, offset]): \tReturns the value of `input` at the row that is the `offset`th row from beginning of the window frame. Offset starts at 1. If ignoreNulls=true, we will skip nulls when finding the `offset`th row. Otherwise, every row counts for the `offset`. If there is no such an `offset`th row (e.g., when the offset is 10, size of the window frame is less than 10), null is returned.\n",
    "\n",
    "- ntile(n): \tDivides the rows for each window partition into `n` buckets ranging from 1 to at most `n`.\n",
    "\n",
    "- percent_rank(): \tComputes the percentage ranking of a value in a group of values.\n",
    "\n",
    "- rank() :\tComputes the rank of a value in a group of values. The result is one plus the number of rows preceding or equal to the current row in the ordering of the partition. The values will produce gaps in the sequence.\n",
    "\n",
    "- row_number(): \tAssigns a unique, sequential number to each row, starting with one, according to the ordering of rows within the window partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Functions\n",
    "\n",
    "\n",
    "- array_contains(array, value): \tReturns true if the array contains the value.\n",
    "- array_distinct(array):\tRemoves duplicate values from the array.\n",
    "- array_except(array1, array2): \tReturns an array of the elements in array1 but not in array2, without duplicates.\n",
    "- array_intersect(array1, array2) :\tReturns an array of the elements in the intersection of array1 and array2, without duplicates.\n",
    "- array_join(array, delimiter[, nullReplacement]): \tConcatenates the elements of the given array using the delimiter and an optional string to replace nulls. If no value is set for nullReplacement, any null value is filtered.\n",
    "- array_max(array): \tReturns the maximum value in the array. NULL elements are skipped.\n",
    "- array_min(array): \tReturns the minimum value in the array. NULL elements are skipped.\n",
    "- array_position(array, element): \tReturns the (1-based) index of the first element of the array as long.\n",
    "- array_remove(array, element): \tRemove all elements that equal to element from array.\n",
    "- array_repeat(element, count): \tReturns the array containing element count times.\n",
    "- array_union(array1, array2): \tReturns an array of the elements in the union of array1 and array2, without duplicates.\n",
    "\n",
    "- arrays_overlap(a1, a2):\tReturns true if a1 contains at least a non-null element present also in a2. If the arrays have no common element and they are both non-empty and either of them contains a null element null is returned, false otherwise.\n",
    "\n",
    "- arrays_zip(a1, a2, ...): \tReturns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n",
    "- concat(col1, col2, ..., colN): \tReturns the concatenation of col1, col2, ..., colN.\n",
    "\n",
    "- flatten(arrayOfArrays):\tTransforms an array of arrays into a single array.\n",
    "\n",
    "- reverse(array): \tReturns a reversed string or an array with reverse order of elements.\n",
    "\n",
    "- sequence(start, stop, step): \tGenerates an array of elements from start to stop (inclusive), incrementing by step. The type of the returned elements is the same as the type of argument expressions. Supported types are: byte, short, integer, long, date, timestamp. The start and stop expressions must resolve to the same type. If start and stop expressions resolve to the 'date' or 'timestamp' type then the step expression must resolve to the 'interval' type, otherwise to the same type as the start and stop expressions.\n",
    "\n",
    "- shuffle(array): \tReturns a random permutation of the given array.\n",
    "\n",
    "- slice(x, start, length): \tSubsets array x starting from index start (array indices start at 1, or starting from the end if start is negative) with the specified length.\n",
    "\n",
    "- sort_array(array[, ascendingOrder]): \tSorts the input array in ascending or descending order according to the natural ordering of the array elements. Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Functions\n",
    "\n",
    "- map_concat(map, ...): \tReturns the union of all the given maps\n",
    "\n",
    "- map_entries(map): \tReturns an unordered array of all entries in the given map.\n",
    "\n",
    "- map_from_entries(arrayOfEntries): \tReturns a map created from the given array of entries.\n",
    "\n",
    "- map_keys(map): \tReturns an unordered array containing the keys of the map.\n",
    "\n",
    "- map_values(map): \tReturns an unordered array containing the values of the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Timestamp Functions\n",
    "\n",
    "- add_months(start_date, num_months): \tReturns the date that is `num_months` after `start_date`.\n",
    "\n",
    "- current_date(): \tReturns the current date at the start of query evaluation. All calls of current_date within the same query return the same value.\n",
    "\n",
    "- current_date: \tReturns the current date at the start of query evaluation.\n",
    "\n",
    "- current_timestamp(): \tReturns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value.\n",
    "\n",
    "- current_timestamp :\tReturns the current timestamp at the start of query evaluation.\n",
    "\n",
    "- current_timezone(): \tReturns the current session local timezone.\n",
    "\n",
    "- date_add(start_date, num_days): \tReturns the date that is `num_days` after `start_date`.\n",
    "\n",
    "- date_format(timestamp, fmt): \tConverts `timestamp` to a value of string in the format specified by the date format `fmt`.\n",
    "\n",
    "- date_from_unix_date(days): \tCreate date from the number of days since 1970-01-01.\n",
    "\n",
    "- date_part(field, source): \tExtracts a part of the date/timestamp or interval source.\n",
    "\n",
    "- date_sub(start_date, num_days): \tReturns the date that is `num_days` before `start_date`.\n",
    "\n",
    "- date_trunc(fmt, ts): \tReturns timestamp `ts` truncated to the unit specified by the format model `fmt`.\n",
    "\n",
    "- datediff(endDate, startDate): \tReturns the number of days from `startDate` to `endDate`.\n",
    "\n",
    "- dayofweek(date): \tReturns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, ..., 7 = Saturday).\n",
    "\n",
    "- dayofyear(date): \tReturns the day of year of the date/timestamp.\n",
    "\n",
    "- from_unixtime(unix_time[, fmt]): \tReturns `unix_time` in the specified `fmt`.\n",
    "\n",
    "- from_utc_timestamp(timestamp, timezone): \tGiven a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1' would yield '2017-07-14 03:40:00.0'.\n",
    "\n",
    "- hour(timestamp): \tReturns the hour component of the string/timestamp.\n",
    "\n",
    "- last_day(date): \tReturns the last day of the month which the date belongs to.\n",
    "\n",
    "- make_date(year, month, day): \tCreate date from year, month and day fields.\n",
    "\n",
    "- make_timestamp(year, month, day, hour, min, sec[, timezone]): \tCreate timestamp from year, month, day, hour, min, sec and timezone fields.\n",
    "\n",
    "- minute(timestamp): \tReturns the minute component of the string/timestamp.\n",
    "\n",
    "- month(date): \tReturns the month component of the date/timestamp.\n",
    "\n",
    "- months_between(timestamp1, timestamp2[, roundOff]): \tIf `timestamp1` is later than `timestamp2`, then the result is positive. If `timestamp1` and `timestamp2` are on the same day of month, or both are the last day of month, time of day will be ignored. Otherwise, the difference is calculated based on 31 days per month, and rounded to 8 digits unless roundOff=false.\n",
    "\n",
    "- next_day(start_date, day_of_week): \tReturns the first date which is later than `start_date` and named as indicated.\n",
    "\n",
    "- now(): \tReturns the current timestamp at the start of query evaluation.\n",
    "\n",
    "- quarter(date): \tReturns the quarter of the year for date, in the range 1 to 4.\n",
    "\n",
    "- second(timestamp): \tReturns the second component of the string/timestamp.\n",
    "\n",
    "- timestamp_micros(microseconds):\tCreates timestamp from the number of microseconds since UTC epoch.\n",
    "\n",
    "- timestamp_millis(milliseconds): \tCreates timestamp from the number of milliseconds since UTC epoch.\n",
    "\n",
    "- timestamp_seconds(seconds) :\tCreates timestamp from the number of seconds (can be fractional) since UTC epoch.\n",
    "\n",
    "- to_date(date_str[, fmt]): \tParses the `date_str` expression with the `fmt` expression to a date. Returns null with invalid input. By default, it follows casting rules to a date if the `fmt` is omitted.\n",
    "\n",
    "- to_timestamp(timestamp_str[, fmt]) :\tParses the `timestamp_str` expression with the `fmt` expression to a timestamp. Returns null with invalid input. By default, it follows casting rules to a timestamp if the `fmt` is omitted.\n",
    "\n",
    "- to_unix_timestamp(timeExp[, fmt]):\tReturns the UNIX timestamp of the given time.\n",
    "\n",
    "- to_utc_timestamp(timestamp, timezone): \tGiven a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield '2017-07-14 01:40:00.0'.\n",
    "\n",
    "- trunc(date, fmt): \tReturns `date` with the time portion of the day truncated to the unit specified by the format model `fmt`.\n",
    "\n",
    "- unix_date(date): \tReturns the number of days since 1970-01-01.\n",
    "\n",
    "- unix_micros(timestamp): \tReturns the number of microseconds since 1970-01-01 00:00:00 UTC.\n",
    "\n",
    "- unix_millis(timestamp): \tReturns the number of milliseconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.\n",
    "\n",
    "- unix_seconds(timestamp): \tReturns the number of seconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.\n",
    "\n",
    "- unix_timestamp([timeExp[, fmt]]): \tReturns the UNIX timestamp of current or specified time.\n",
    "\n",
    "- weekday(date): \tReturns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\n",
    "\n",
    "- weekofyear(date): \tReturns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with >3 days.\n",
    "\n",
    "- year(date): \tReturns the year component of the date/timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Functions\n",
    "\n",
    "- from_json(jsonStr, schema[, options]): \tReturns a struct value with the given `jsonStr` and `schema`.\n",
    "- get_json_object(json_txt, path): \tExtracts a json object from `path`.\n",
    "- json_array_length(jsonArray): \tReturns the number of elements in the outmost JSON array.\n",
    "- json_object_keys(json_object): \tReturns all the keys of the outmost JSON object as an array.\n",
    "- json_tuple(jsonStr, p1, p2, ..., pn): \tReturns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.\n",
    "\n",
    "- schema_of_json(json[, options]): \tReturns schema in the DDL format of JSON string.\n",
    "- to_json(expr[, options]): \tReturns a JSON string with a given struct value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar User Defined Functions (UDFs)\n",
    "\n",
    "Scalar User-Defined Functions (UDFs) are user-programmable routines that act on one row. This documentation lists the classes that are required for creating and registering UDFs. It also contains examples that demonstrate how to define and register UDFs and invoke them in Spark SQL.\n",
    "\n",
    "Available only in java & Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Aggregate Functions (UDAFs)\n",
    "\n",
    "User-Defined Aggregate Functions (UDAFs) are user-programmable routines that act on multiple rows at once and return a single aggregated value as a result. This documentation lists the classes that are required for creating and registering UDAFs. It also contains examples that demonstrate how to define and register UDAFs in Scala and invoke them in Spark SQL.m\n",
    "Available only in java & Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers : An identifier is a string used to identify a database object such as a table, view, schema, column, etc. Spark SQL has regular identifiers and delimited identifiers, which are enclosed within backticks. Both regular identifiers and delimited identifiers are case-insensitive\n",
    "\n",
    "\n",
    "Literals : A literal (also known as a constant) represents a fixed data value. Spark SQL supports the following literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Definition Statements\n",
    "- ALTER DATABASE\n",
    "- ALTER TABLE\n",
    "- ALTER VIEW\n",
    "- CREATE DATABASE\n",
    "- CREATE FUNCTION\n",
    "- CREATE TABLE\n",
    "- CREATE VIEW\n",
    "- DROP DATABASE\n",
    "- DROP FUNCTION\n",
    "- DROP TABLE\n",
    "- DROP VIEW\n",
    "- TRUNCATE TABLE\n",
    "- REPAIR TABLE\n",
    "- USE DATABASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation Statements\n",
    "\n",
    "- INSERT \n",
    "    - INSERT INTO statement\n",
    "    - INSERT OVERWRITE statement\n",
    "    - INSERT OVERWRITE DIRECTORY statement\n",
    "    - INSERT OVERWRITE DIRECTORY with Hive format statement\n",
    "    \n",
    "- LOAD\n",
    "    - LOAD DATA statement loads the data into a Hive serde table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval\n",
    "\n",
    "\n",
    "- SELECT Statement : retrieve rows from one or more tables according to the specified clauses\n",
    "     - WHERE Clause\n",
    "     - GROUP BY Clause\n",
    "     - HAVING Clause\n",
    "     - ORDER BY Clause\n",
    "     - SORT BY Clause\n",
    "     - CLUSTER BY Clause\n",
    "     - DISTRIBUTE BY Clause\n",
    "     - LIMIT Clause\n",
    "     - Common Table Expression\n",
    "     - Hints\n",
    "     - Inline Table\n",
    "     - File\n",
    "     - JOIN\n",
    "     - LIKE Predicate\n",
    "     - Set Operators\n",
    "     - TABLESAMPLE\n",
    "     - Table-valued Function\n",
    "     - Window Function\n",
    "     - CASE Clause\n",
    "     - PIVOT Clause\n",
    "     - LATERAL VIEW Clause\n",
    "     - TRANSFORM Clause\n",
    "     \n",
    "- EXPLAIN Statement : generate logical and physical plan for a given query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Statements\n",
    "\n",
    "- ANALYZE : The ANALYZE TABLE statement collects statistics about the table to be used by the query optimizer to find a better query execution plan.\n",
    "\n",
    "- CACHE :\n",
    "    - CACHE TABLE\n",
    "    - UNCACHE TABLE\n",
    "    - CLEAR CACHE\n",
    "    - REFRESH TABLE\n",
    "    - REFRESH FUNCTION\n",
    "    - REFRESH\n",
    "\n",
    "- DESCRIBE :\n",
    "    - DESCRIBE DATABASE\n",
    "    - DESCRIBE TABLE\n",
    "    - DESCRIBE FUNCTION\n",
    "    - DESCRIBE QUERY\n",
    "\n",
    "- SHOW : \n",
    "   -  SHOW COLUMNS\n",
    "   -  SHOW CREATE TABLE\n",
    "   -  SHOW DATABASES\n",
    "   -  SHOW FUNCTIONS\n",
    "   -  SHOW PARTITIONS\n",
    "   -  SHOW TABLE EXTENDED\n",
    "   -  SHOW TABLES\n",
    "   -  SHOW TBLPROPERTIES\n",
    "   -  SHOW VIEWS\n",
    "\n",
    "\n",
    "- CONFIGURATION MANAGEMENT \n",
    "    - SET\n",
    "    - RESET\n",
    "    - SET TIME ZONE\n",
    "\n",
    "- RESOURCE MANAGEMENT\n",
    "    - ADD FILE\n",
    "    - ADD JAR\n",
    "    - LIST FILE\n",
    "    - LIST JAR\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
