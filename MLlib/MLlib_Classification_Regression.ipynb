{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_version = \"2.4.7\"\n",
    "spark_home = \"file:///Users/kaustuv/spark-2.4.7/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models( DF)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession  to SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial logistic regression\n",
    "\n",
    "Following example shows how to train binomial and multinomial logistic regression models for binary classification with elastic net regularization. elasticNetParam corresponds to α and regParam corresponds to λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.353983524188197e-05,-9.102738505589466e-05,-0.00019467430546904298,-0.00020300642473486668,-3.1476183314863995e-05,-6.842977602660743e-05,1.5883626898239883e-05,1.4023497091372047e-05,0.00035432047524968605,0.00011443272898171087,0.00010016712383666666,0.0006014109303795481,0.0002840248179122762,-0.00011541084736508837,0.000385996886312906,0.000635019557424107,-0.00011506412384575676,-0.00015271865864986808,0.0002804933808994214,0.0006070117471191634,-0.0002008459663247437,-0.0001421075579290126,0.0002739010341160883,0.00027730456244968115,-9.838027027269332e-05,-0.0003808522443517704,-0.00025315198008555033,0.00027747714770754307,-0.0002443619763919199,-0.0015394744687597765,-0.00023073328411331293])\n",
      "Intercept: 0.22456315961250325\n",
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.12065879445860686,0.12065879445860686]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegressionTrainingSummary provides a summary for a LogisticRegressionModel. In the case of binary classification, certain additional metrics are available, e.g. ROC curve. See BinaryLogisticRegressionTrainingSummary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6662875751473734\n",
      "0.6217068546034618\n",
      "0.6127265245887887\n",
      "0.6060347986802873\n",
      "0.6031750687571562\n",
      "0.5969621534836274\n",
      "0.5940743031983118\n",
      "0.5906089243339022\n",
      "0.5894724576491042\n",
      "0.5882187775729587\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_4ae6339abea8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial logistic regression\n",
    "Below example shows how to train a multiclass logistic regression model with elastic net regularization, as well as extract the multiclass training summary for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 X 4 CSRMatrix\n",
      "(0,3) 0.3176\n",
      "(1,2) -0.7804\n",
      "(1,3) -0.377\n",
      "Intercept: [0.05165231659832854,-0.12391224990853622,0.07225993331020768]\n",
      "objectiveHistory:\n",
      "1.098612288668108\n",
      "1.087602085441699\n",
      "1.0341156572156232\n",
      "1.0289859520256006\n",
      "1.0300389657358995\n",
      "1.0239965158223991\n",
      "1.0236097451839508\n",
      "1.0231082121970012\n",
      "1.023022220302788\n",
      "1.0230018151780262\n",
      "1.0229963739557606\n",
      "False positive rate by label:\n",
      "label 0: 0.22\n",
      "label 1: 0.05\n",
      "label 2: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 0.46\n",
      "Precision by label:\n",
      "label 0: 0.6944444444444444\n",
      "label 1: 0.9090909090909091\n",
      "label 2: 1.0\n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 0.46\n",
      "F-measure by label:\n",
      "label 0: 0.819672131147541\n",
      "label 1: 0.9523809523809523\n",
      "label 2: 0.6301369863013699\n",
      "Accuracy: 0.82\n",
      "FPR: 0.09\n",
      "TPR: 0.82\n",
      "F-measure: 0.8007300232766211\n",
      "Precision: 0.8678451178451179\n",
      "Recall: 0.82\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark \\\n",
    "    .read \\\n",
    "    .format(\"libsvm\") \\\n",
    "    .load(spark_home +\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[121,122,123...|\n",
      "|       1.0|         1.0|(692,[122,123,148...|\n",
      "|       0.0|         1.0|(692,[126,127,128...|\n",
      "|       0.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.0967742 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_f3476ed3adec) of depth 1 with 3 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[98,99,100,1...|\n",
      "|           0.0|  0.0|(692,[100,101,102...|\n",
      "|           0.0|  0.0|(692,[121,122,123...|\n",
      "|           0.0|  0.0|(692,[123,124,125...|\n",
      "|           0.0|  0.0|(692,[123,124,125...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_0396ffa6085d) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient-boosted tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[95,96,97,12...|\n",
      "|       1.0|         1.0|(692,[122,123,148...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[125,126,127...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.0606061\n",
      "GBTClassificationModel (uid=GBTClassifier_d5cd4f62cfef) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home + \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9019607843137255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(spark_home +\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [4, 5, 4, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005170630317473439,-0.0001172288654973735,-8.882754836918948e-05,8.522360710187464e-05,0.0,0.0,-1.3436361263314267e-05,0.0003729569801338091,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0008888949552633658,0.00029864059761812683,0.0003793378816193159,-0.0001762328898254081,0.0,1.5028489269747836e-06,1.8056041144946687e-06,1.8028763260398597e-06,-3.3843713506473646e-06,-4.041580184807502e-06,2.0965017727015125e-06,8.536111642989494e-05,0.00022064177429604464,0.00021677599940575452,-0.0005472401396558763,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000921415502407147,0.00031351066886882195,0.0002481984318412822,0.0,-4.147738197636148e-05,-3.6832150384497175e-05,0.0,-3.9652366184583814e-06,-5.1569169804965594e-05,-6.624697287084958e-05,-2.182148650424713e-05,1.163442969067449e-05,-1.1535211416971104e-06,3.8138960488857075e-05,1.5823711634321492e-06,-4.784013432336632e-05,-9.386493224111833e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00043174897827077767,0.00017055492867397665,0.0,-2.7978204136148868e-05,-5.88745220385208e-05,-4.1858794529775e-05,-3.740692964881002e-05,-3.9787939304887e-05,-5.545881895011037e-05,-4.505015598421474e-05,-3.214002494749943e-06,-1.6561868808274739e-06,-4.416063987619447e-06,-7.9986183315327e-06,-4.729962112535003e-05,-2.516595625914463e-05,-3.6407809279248066e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00024719098130614967,0.0,-3.270637431382939e-05,-5.5703407875748054e-05,-5.2336892125702286e-05,-7.829604482365818e-05,-7.60385448387619e-05,-8.371051301348216e-05,-1.8669558753795108e-05,0.0,1.2045309486213725e-05,-2.3374084977016397e-05,-1.0788641688879534e-05,-5.5731194431606874e-05,-7.952979033591137e-05,-1.4529196775456057e-05,8.737948348132623e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0012589360772978808,-0.0001816228630214369,-0.00010650711664557365,-6.040355527710781e-05,-4.856392973921569e-05,-8.973895954652451e-05,-8.78131677062384e-05,-5.68487774673792e-05,-3.780926734276347e-05,1.3834897036553787e-05,7.585485129441565e-05,5.5017411816753975e-05,-1.5430755398169695e-05,-1.834928703625931e-05,-0.00010354008265646844,-0.00013527847721351194,-0.00011245007647684532,-2.9373916056750564e-05,-7.311217847336934e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002858228613863785,-0.00012998173971449976,-0.0001478408021316135,-8.203374605865772e-05,-6.556685320008032e-05,-5.6392660386580244e-05,-6.995571627330911e-05,-4.664348159856693e-05,-2.3026593698824318e-05,7.398833979172035e-05,0.00014817176130099997,0.00010938317435545486,7.940425167011364e-05,-6.743294804348106e-07,-0.00012623302721464762,-0.00019110387355357616,-0.00018611622108961136,-0.00012776766254736952,-8.935302806524433e-05,-1.239417230441996e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002829530831354112,-0.00013912189600461263,-0.00012593136464577562,-5.964745187930992e-05,-5.360328152341982e-05,-0.00010517880662090183,-0.00013856124131005022,-7.181032974125911e-05,2.3249038865093483e-06,0.0001566964269571967,0.00023261206954040812,0.00017261638232256968,0.00013857530960270466,-1.396299028868332e-05,-0.00015765773982418597,-0.00020728798812007546,-0.00019106441272002828,-0.00012744834161431415,-0.00012755611630280015,-5.1885591560478935e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000159081567023441,-0.0001216531230287931,-5.623851079809818e-05,-3.877987126382982e-05,-7.550900509956966e-05,-0.00010703140005463545,-0.00014720428138106226,-8.781423374509368e-05,7.941655609421792e-05,0.00023206354986219992,0.00027506982343672394,0.0002546722233188043,0.0001810821666388498,-1.3069916689929984e-05,-0.0001842374220886751,-0.0001977540482445517,-0.00017722074063670741,-0.0001487987014723575,-0.00011879021431288621,-9.755283887790393e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0001302740311359312,-5.3683030235535024e-05,-1.7631200013656873e-05,-7.846611034608254e-05,-0.000122100767283256,-0.00017281968533449702,-0.00015592346128894157,-5.239579492910452e-05,0.0001680719343542442,0.00028930086786548053,0.0003629921493231646,0.0002958223512266975,0.00021770466955449064,-6.40884808188951e-05,-0.00019058225556007997,-0.00020425138564600712,-0.0001711994903702119,-0.00013853486798341369,-0.00013018592950855062,-0.00011887779512760102,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.021411112285498e-05,-1.694500843168125e-05,-7.189722824172193e-05,-0.00014560828004346436,-0.00014935497340563198,-0.00019496419340776972,-0.00017383743417254187,-3.3438825792010694e-05,0.0002866538327947017,0.00029812321570739803,0.000377250607691119,0.0003211702827486386,0.0002577995115175486,-0.00016627385656703205,-0.00018037105851523224,-0.00020419356344211325,-0.00017962237203420184,-0.00013726488083579862,-0.00013461014473741762,-0.00012264216469164138,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0015239752514658556,-5.472330865993813e-05,-9.65684394936216e-05,-0.00013424729853486994,-0.00014727467799568,-0.0001616270978824712,-0.00018458259010029364,-0.00019699647135089726,0.00013085261294290817,0.0002943178857107149,0.0003097773692834126,0.0004112834769312103,0.00034113620757035025,0.00016529945924367265,-0.00021065410862650534,-0.0001883924081539624,-0.0001979586414569358,-0.0001762131187223702,-0.0001272343622678854,-0.00012708161719220297,-0.00014812221011889967,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.001140680600536578,-0.0001323467421269896,-0.00012904607854274846,-0.00014104748544921958,-0.00015194605434027872,-0.00021104539389774283,-0.00017911827582001795,-0.00018952948277194435,0.00021767571552539842,0.00030201791656326465,0.0004002863274397723,0.00040322806756364006,0.0004118077382608461,3.7917405252859545e-06,-0.00019886290660234838,-0.00019547443112937263,-0.00019857348218680872,-0.00013336892200703206,-0.00012830129292910815,-0.00011855916317355505,-0.0001765597203760205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010938769592297973,-0.00012785475305234688,-0.00013424699777466666,-0.0001505200652479287,-0.00019333287822872713,-0.00020385160086594937,-0.00017422470698847553,4.63598443910652e-05,0.00020617623087127652,0.0002862882891134514,0.0004074830988361515,0.0003726357785147985,0.0003507520190729629,-0.0001516485494364312,-0.00017053751921469217,-0.00019638964654350848,-0.00019962586265806435,-0.00013612312664311173,-0.0001218285533892454,-0.00011166712081624676,-0.0001377283888177579,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003044386260118809,-0.0001240836643202059,-0.0001335317492716633,-0.00015783442604618277,-0.00019168434243384107,-0.00018710322733892716,-0.00011283989231463139,0.00011136504453105364,0.00018707244892705632,0.00028654279528966305,0.00040032117544983536,0.0003169637536305377,0.00020158994278679014,-0.00013139392844616033,-0.00015181070482383948,-0.0001825431845981843,-0.0001602539928567571,-0.00013230404795396355,-0.00011669138691257469,-0.00010532154964150405,-0.00013709037042366007,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00040287410145021705,-0.00013563987950912995,-0.00013225887084018914,-0.00016523502389794188,-0.00020175074284706945,-0.0001572459106394481,2.577536501278673e-06,0.0001312463663419457,0.00020707422291927531,0.00039081065544314936,0.00033487058329898135,0.00025790441367156086,2.6881819648016494e-05,-0.0001511383586714907,-0.0001605428139328567,-0.00017267287462873575,-0.00011938943768052963,-0.00010505245038633314,-0.00011109385509034013,-0.00013469914274864725,-0.00020735223736035555,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005034374233912422,-0.00015961213688405883,-0.0001274222123810994,-0.0001582821104884909,-0.00021301220616286252,-0.00012933366375029613,1.6802673102179614e-05,0.00011020918082727098,0.00021160795272688753,0.00034873421050827716,0.00026487211944380384,0.0001151606835026639,-5.4682731396851946e-05,-0.00013632001630934325,-0.00014340405857651405,-0.0001248695773821634,-8.462873247977974e-05,-9.580708414770257e-05,-0.00010749166605399431,-0.00014618038459197777,-0.00037556446296204636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005124342611878493,-0.00020369734099093433,-0.00013626985098328694,-0.00013313768183302705,-0.0001871555537819396,-0.0001188817315789655,-1.8774817595622694e-05,5.7108412194993384e-05,0.00012728161056121406,0.00019021458214915667,0.00012177397895874969,-1.2461153574281128e-05,-7.553961810487739e-05,-0.00010242174559410404,-4.44873554195981e-05,-9.058561577961895e-05,-6.837347198855518e-05,-8.084409304255458e-05,-0.00013316868299585082,-0.00020335916397646626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003966510928472775,-0.00013738983629066386,-3.7971221409699866e-05,-6.431763035574533e-05,-0.00011857739882295322,-9.359520863114822e-05,-5.0878371516215046e-05,-8.269367595092908e-08,0.0,1.3434539131099211e-05,-1.9601690213728576e-06,-2.8527045990494954e-05,-7.410332699310603e-05,-7.132130570080122e-05,-4.9780961185536e-05,-6.641505361384578e-05,-6.962005514093816e-05,-7.752898158331023e-05,-0.00017393609499225025,-0.0012529479255443958,0.0,0.0,0.00020682521269893754,0.0,0.0,0.0,0.0,0.0,-0.00046702467383631055,-0.00010318036388792008,1.2004408785841247e-05,0.0,-2.5158639357650687e-05,-1.2095240910793449e-05,-5.19052816902203e-06,-4.916790639558058e-06,-8.48395853563783e-06,-9.362757097074547e-06,-2.0959335712838412e-05,-4.7790091043859085e-05,-7.92797600958695e-05,-4.462687041778011e-05,-4.182992428577707e-05,-3.7547996285851254e-05,-4.52754480225615e-05,-1.8553562561513456e-05,-0.00024763037962085644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00034886180455242474,-5.687523659359091e-06,7.380040279654313e-05,4.395860636703821e-05,7.145198242379862e-05,6.181248343370637e-06,0.0,-6.0855538083486296e-05,-4.8563908323274725e-05,-4.117920588930435e-05,-4.359283623112936e-05,-6.608754161500044e-05,-5.443032251266018e-05,-2.7782637880987207e-05,0.0,0.0,0.0002879461393464088,-0.0028955529777851255,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00012312114837837392,-1.9526747917254753e-05,-1.6999506829961688e-05,5.4835294148085086e-05,1.523441632762399e-05,-5.8365604525328614e-05,-0.00012378194216521848,-0.00011750704953254656,-6.19711523061306e-05,-5.042009645812091e-05,-0.00014055260223565886,-0.0001410330942465528,-0.00019272308238929396,-0.0004802489964676616]\n",
      "Intercept: 0.012911305214513969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear SVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-Rest classifier (a.k.a. One-vs-All) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# load data file.\n",
    "inputData = spark.read.format(\"libsvm\") \\\n",
    "    .load(spark_home +\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# generate the train/test split.\n",
    "(train, test) = inputData.randomSplit([0.8, 0.2])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|label|            features|       rawPrediction|probability|prediction|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[-174115.98587057...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[98,99,100,1...|[-178402.52307196...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[100,101,102...|[-100905.88974016...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-244784.29791241...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-196900.88506109...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-238164.45338794...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-184206.87833381...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-214174.52863813...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-182844.62193963...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[-246557.10990301...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-208282.08496711...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-243457.69885665...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[-260933.50931276...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[-220274.72552901...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[-154830.07125175...|  [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-145978.24563975...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[100,101,102...|[-147916.32657832...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-139663.27471685...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-129013.44238751...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-81829.799906049...|  [0.0,1.0]|       1.0|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\") \\\n",
    "    .load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Factorization machines classifier\n",
    " \n",
    " - Factorization Machines are able to estimate interactions between features even in problems with huge sparsity (like advertising and recommendation system). The spark.ml implementation supports factorization machines for binary classification and for regression.\n",
    " \n",
    "- FM can be used for regression and optimization criterion is mean square error. FM also can be used for binary classification through sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[100,101,102...|\n",
      "|       1.0|         1.0|(692,[122,123,148...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[125,126,127...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test set accuracy = 1\n",
      "Factors: DenseMatrix([[-0.00528012,  0.00143181,  0.01013465, ...,  0.01015624,\n",
      "              -0.01110619,  0.01519348],\n",
      "             [-0.01187346,  0.01664713, -0.00942243, ...,  0.01208724,\n",
      "              -0.02360421,  0.00613407],\n",
      "             [ 0.00893668, -0.01061823, -0.00439162, ...,  0.02286302,\n",
      "               0.01084765,  0.00264926],\n",
      "             ...,\n",
      "             [ 0.03381106,  0.01313241,  0.02041935, ...,  0.03890654,\n",
      "              -0.02399354,  0.03023109],\n",
      "             [-0.0007612 ,  0.02863742,  0.03530331, ...,  0.01534508,\n",
      "              -0.01842559,  0.01267808],\n",
      "             [ 0.04652608,  0.02239231,  0.02263293, ...,  0.02878369,\n",
      "              -0.03296062,  0.02074721]])\n",
      "Linear: [-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.014923586235272018,0.014923586235272018,-0.024479199184202317,-0.032806051720968385,-0.032669825499861714,-0.033260236158382805,-0.020371222620730774,0.0,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.029344878151944552,-0.02934487815194455,-0.0318354062209845,0.010745487851296903,0.008186083043867087,-0.01967350332148922,-0.027921765511133297,-0.028744714501173215,-0.02743801016098505,-0.029115315648380123,-0.02974674763865637,-0.030355138979794864,-0.03129008444902584,-0.031275005666866064,0.01141398245231238,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.029344878151944552,-0.029344878151944552,-0.029344878151944552,-0.03247503931563829,0.0041905773650682595,0.00922258864747072,-0.02041881004722085,-0.025059528145700564,-0.01212392526916993,-0.010813065069560946,-0.018919989802807593,-0.0287245854118074,-0.027844668412566692,-0.029797211966898406,-0.012341912467503359,-0.0017435207383114397,0.011214197465318293,0.010711619064067417,0.010711619064067417,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.029344878151944552,-0.030458211609843466,-0.034081506412997675,0.001700880817844384,0.005576112262155134,0.0006674914062068574,-0.011599750982937102,-0.014900832998570486,-0.018902430131775873,-0.02160511208524213,-0.02653378131287206,-0.027578937259795037,-0.025377372328371114,-0.01865958948274237,0.005768782399872678,0.0032232143544239818,0.009951045397559928,0.010711619064067417,0.010711619064067417,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.008926826651993769,-0.028882178021809975,0.0036851762400101516,0.006170521882932554,0.0033939613528799727,0.004319396204507514,-0.005965106997340447,-0.010622778725886824,-0.02693153127494515,-0.02940731279213784,-0.029410068076868988,-0.02540978088325734,-0.023980021386036412,-0.005114144804089364,0.0075124000390661425,0.0021784693974641668,-0.0008484747781807694,0.010820455545607033,0.010711619064067417,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.0,0.018383709457187616,0.015471282903757946,0.008494241047000728,0.0030895762489495998,0.008304910727406968,0.0074577404889883,-0.008522277907313443,-0.022793348883397308,-0.03081747047209205,-0.030043983490500747,-0.02835236223670231,-0.027327962555304012,-0.02305845403379668,0.007406290580049498,0.010878694026505632,0.01054150246704179,0.005591777086022342,0.011230252133541962,0.010711619064067417,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.0,0.017771280087666163,0.014885692691610317,0.01104267649913109,0.009778029377132829,0.010105400605429839,0.010657560985669898,-0.015506027871250352,-0.027608432603368035,-0.029716083290114374,-0.027607149019182743,-0.026534054251984254,-0.028165497001601085,-0.027235644826838226,0.01319803667423444,0.01634164034204263,0.016936369109775178,0.015680382920428032,0.011674768348577651,0.01073857525772531,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.017532969959020376,0.015898980306930266,0.014452201573146792,0.013329257748125167,0.012801763095405412,0.01096663279882758,0.005608421453954475,-0.017801709059404563,-0.028818736193574918,-0.027781506573368208,-0.02650856697003656,-0.02589766829289138,-0.02827460385024702,-0.028399785811924604,0.010473731161036165,0.01624718294006899,0.017560666479693638,0.019044546837350535,0.021931774184863608,0.014662853347837181,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01565590722784345,0.017169688580055884,0.015032036169325165,0.011930660480557072,0.011661285129444544,0.00737164489326956,0.007825123753411491,-0.015400509653149022,-0.029196506209895466,-0.026003442346078084,-0.02563721453080002,-0.025645211737300593,-0.027517106797638535,-0.028027089723648247,0.014354368571586507,0.015638496303132958,0.014660072529815328,0.01701332979874356,0.021452182982581034,0.024414335199948664,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.017110756436499792,0.017355801198075768,0.007039897604557464,0.00840914758152645,0.012313449615534176,0.01188896228614349,0.008424230586923855,-0.027077218666576006,-0.0277845752142325,-0.025993084372148402,-0.025687240191930086,-0.02598930935547182,-0.028160433841133887,0.0007345470021998189,0.016493493957338543,0.015878734535059048,0.0143102283874615,0.013760009709109991,0.019431372069184685,0.02636201048193968,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.020865202476712048,0.013961811315997321,0.010321156581828562,0.01440121698087876,0.015113771894259343,0.014013926000040514,0.012484868747296261,-0.0309390011444699,-0.025981075960671226,-0.02561803086242131,-0.025694454586923286,-0.025883986839457425,-0.02919552627862978,0.014443341955121432,0.01573551935375205,0.016192383397838742,0.01468592286015541,0.013471880938953329,0.017480411959055274,0.025892490348094266,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01254890343153268,0.021960989810110722,0.015550459847637223,0.013229876232284074,0.015272685981032732,0.015065669087945455,0.014609825133041281,0.013056448283567819,-0.02618620489436177,-0.025372193631805533,-0.025506167923013957,-0.025789459258837463,-0.0262350853300877,-0.030608447453799374,0.01691921242282254,0.015653115909478803,0.016370958557586635,0.015261235572378124,0.013090208594938731,0.017485876033755787,0.024302291696317334,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012381695263073745,0.02267858649746216,0.015473390153308664,0.013553876511008279,0.015213239326082742,0.015454072460027822,0.014561698384835979,-0.01632014807935225,-0.024737850755306483,-0.025213932300664047,-0.025788307908301073,-0.025851745735276828,-0.027469524155087407,-0.030260019383571823,0.015020931707458043,0.015424927602546172,0.016198418611900713,0.014512530582809068,0.01567001164755262,0.020263116549495523,0.02269536230638504,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012006842004336831,0.020458241583818197,0.016113973349847033,0.014073936315424494,0.015367396617945523,0.015495486559237768,0.011535496684339555,-0.026208936485106166,-0.024581962249082406,-0.02513369080062512,-0.02592717494371407,-0.026225045812243514,-0.02718305393826372,-0.024546055982202535,0.011607766381690669,0.014322202777018293,0.01556042930734067,0.017028824596631915,0.018588310815847053,0.016920231677255287,0.015905524257294523,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012486142751756885,0.019919039877580407,0.016584226806260684,0.014246648564205199,0.015484806280467672,0.015195244969409018,-0.01929045362509737,-0.025047254547440088,-0.024543935602487923,-0.025134952016666573,-0.02599301407006081,-0.026795490451837604,-0.029128313647314653,-0.0019456783873973242,0.009552452521855144,0.012916734593518234,0.01708791723295735,0.018123249043671318,0.015552305654024729,0.013930103231875737,0.015052717164445952,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01247283939437584,0.0169965813597192,0.018217070683898964,0.015403897917689386,0.015444038930088048,0.013486604157811005,-0.028003072610980655,-0.024637920910310616,-0.02480013603359975,-0.025763942653156558,-0.026728941668648535,-0.02794322898378193,-0.03169273110796716,0.008453697420649987,0.013536237262337672,0.014411294741184553,0.014054378368542047,0.012850344725038786,0.013194657748521403,0.015519295192410573,0.01616738994660199,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012440729706300316,0.013534586635348555,0.017066210005197376,0.01720997659457623,0.016513000995419753,0.007148545412920818,-0.02860092460316251,-0.026181080192379913,-0.025418191500024397,-0.02639071135534965,-0.027375042283057333,-0.028886391440817318,-0.017718546519011058,0.004896275672219966,0.007130302551521029,0.007376405513919845,0.007357995518328114,0.010251210495681331,0.013967148998117499,0.01638059400068565,0.016261849064745808,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012603138857485595,0.012762563881681132,0.01293256054259906,0.012197881013111653,0.01347399373372368,0.007488627611370362,-0.023591336402562585,-0.026556421256402604,-0.026565375575435417,-0.028941850390616437,-0.030813193861044395,-0.025920145903249328,-0.010684256539463472,0.0007190327705140611,0.0024920529719655313,0.00945399006474678,0.005670047551836254,0.007933896654347468,0.015712553003404076,0.016519573214917548,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01274398553437676,0.0126698181668727,0.00021832303404878864,0.004538304783060243,0.004340315539244299,-0.007014640837801235,-0.023088619121823622,-0.027767882357015803,-0.027333429722417407,-0.02973102772036331,-0.027315590735265818,-0.01822155349135801,-0.00601915932019643,-0.0004405833628260432,0.007279653086090974,0.009664924630906608,0.0073792508993257705,0.008940380419190909,0.016727331603110573,0.01610359154948319,-0.030433941293391546,-0.030433941293391546,-0.023164764412720865,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01274398553437676,0.01274398553437676,-0.022934918807097467,-0.015943921757375825,-0.0182545235624925,-0.023582626809684478,-0.02325006675927402,-0.02314273492736553,-0.020160859854326942,-0.019900292933196072,-0.013904018596374364,-0.008314649969512188,-0.000651366003979257,-0.0012315937736929745,-0.0033967539924152665,-0.007816861611754015,0.002273764260040442,-0.008752690155422412,0.017515119119221472,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.01274398553437676,0.007015659550205413,-0.027798149454969178,-0.03291060304837971,-0.03172851000396785,-0.02780071778258726,-0.0167959400044929,-0.0017438500865694594,-0.00448395932358156,0.00011656784140631583,0.0033266467963526383,0.00033482738967221073,-0.013122617575327216,-0.02308088052764954,-0.029584555941540185,-0.034282520987993385,-0.030248449146487333,0.0,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,-0.030433941293391546,0.012743982636434318,0.01274398553437676,0.012795443400437456,-0.028392456816817152,-0.029278010963617766,-0.005553094589146038,0.011866812318297565,0.01878966459209747,0.019451493595487198,0.019742400874652104,0.019389225602869396,0.01771218942755483,0.017956890977794845,0.01858152798169597]\n",
      "Intercept: -0.030433945017932004\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"file:///Users/kaustuv/spark-3.1.1/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Scale features.\n",
    "featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a FM model.\n",
    "fm = FMClassifier(labelCol=\"indexedLabel\", featuresCol=\"scaledFeatures\", stepSize=0.001)\n",
    "\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureScaler, fm])\n",
    "\n",
    "# Train model.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = %g\" % accuracy)\n",
    "\n",
    "fmModel = model.stages[2]\n",
    "print(\"Factors: \" + str(fmModel.factors))  # type: ignore\n",
    "print(\"Linear: \" + str(fmModel.linear))  # type: ignore\n",
    "print(\"Intercept: \" + str(fmModel.intercept))  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(spark_home +\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.010541828081257216,0.8003253100560949,-0.7845165541420371,2.3679887171421914,0.5010002089857577,1.1222351159753026,-0.2926824398623296,-0.49837174323213035,-0.6035797180675657,0.6725550067187461]\n",
      "Intercept: 0.14592176145232041\n",
      "Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.8312649247659919, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]\n",
      "T Values: [0.013259446542269243, 0.9942283563442594, -0.9836067393599172, 2.848657084633759, 0.6305509179635714, 1.382234441029355, -0.3695715687490668, -0.6250446546128238, -0.7271418403049983, 0.8654306337661122, 0.31453393176593286]\n",
      "P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]\n",
      "Dispersion: 105.60988356821714\n",
      "Null Deviance: 53229.3654338832\n",
      "Residual Degree Of Freedom Null: 500\n",
      "Deviance: 51748.8429484264\n",
      "Residual Degree Of Freedom: 490\n",
      "AIC: 3769.1895871765314\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-10.974359174246889|\n",
      "| 0.8872320138420559|\n",
      "| -4.596541837478908|\n",
      "|-20.411667435019638|\n",
      "|-10.270419345342642|\n",
      "|-6.0156058956799905|\n",
      "|-10.663939415849267|\n",
      "| 2.1153960525024713|\n",
      "| 3.9807132379137675|\n",
      "|-17.225218272069533|\n",
      "| -4.611647633532147|\n",
      "| 6.4176669407698546|\n",
      "| 11.407137945300537|\n",
      "| -20.70176540467664|\n",
      "| -2.683748540510967|\n",
      "|-16.755494794232536|\n",
      "|  8.154668342638725|\n",
      "|-1.4355057987358848|\n",
      "|-0.6435058688185704|\n",
      "|  -1.13802589316832|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Load training data\n",
    "dataset = spark.read.format(\"libsvm\")\\\n",
    "    .load(spark_home+ \"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[100,101,102...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.288675\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_dbb2aec4d00a) of depth 1 with 3 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[121,122,123...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.0647393\n",
      "RandomForestRegressionModel (uid=RandomForestRegressor_7126e9fb1c69) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.204124\n",
      "GBTRegressionModel (uid=GBTRegressor_a542933520e0) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(spark_home+ \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Survival regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.4963111466650681,0.19844437699933626]\n",
      "Intercept: 2.6380946151040057\n",
      "Scale: 1.5472345574364688\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|label|censor|features      |prediction        |quantiles                              |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|1.218|1.0   |[1.56,-0.605] |5.718979487634987 |[1.1603238947151628,4.9954560102747525]|\n",
      "|2.949|0.0   |[0.346,2.158] |18.076521181495476|[3.6675458454717704,15.78961186627775] |\n",
      "|3.627|0.0   |[1.38,0.231]  |7.381861804239099 |[1.4977061305190842,6.447962612338963] |\n",
      "|0.273|1.0   |[0.52,1.151]  |13.577612501425325|[2.754762148150695,11.859872224069736] |\n",
      "|4.199|0.0   |[0.795,-0.226]|9.013097744073866 |[1.828667632129777,7.872826505878401]  |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (1.218, 1.0, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, 0.0, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, 0.0, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, 1.0, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, 0.0, Vectors.dense(0.795, -0.226))], [\"label\", \"censor\", \"features\"])\n",
    "quantileProbabilities = [0.3, 0.6]\n",
    "aft = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "\n",
    "model = aft.fit(training)\n",
    "\n",
    "# Print the coefficients, intercept and scale parameter for AFT survival regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "print(\"Scale: \" + str(model.scale))\n",
    "model.transform(training).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotonic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundaries in increasing order: [0.01,0.17,0.18,0.27,0.28,0.29,0.3,0.31,0.34,0.35,0.36,0.41,0.42,0.71,0.72,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,1.0]\n",
      "\n",
      "Predictions associated with the boundaries: [0.15715271294117644,0.15715271294117644,0.189138196,0.189138196,0.20040796,0.29576747,0.43396226,0.5081591025000001,0.5081591025000001,0.54156043,0.5504844466666667,0.5504844466666667,0.563929967,0.563929967,0.5660377366666667,0.5660377366666667,0.56603774,0.57929628,0.64762876,0.66241713,0.67210607,0.67210607,0.674655785,0.674655785,0.73890872,0.73992861,0.84242733,0.89673636,0.89673636,0.90719021,0.9272055075,0.9272055075]\n",
      "\n",
      "+----------+--------------+-------------------+\n",
      "|     label|      features|         prediction|\n",
      "+----------+--------------+-------------------+\n",
      "|0.24579296|(1,[0],[0.01])|0.15715271294117644|\n",
      "|0.28505864|(1,[0],[0.02])|0.15715271294117644|\n",
      "|0.31208567|(1,[0],[0.03])|0.15715271294117644|\n",
      "|0.35900051|(1,[0],[0.04])|0.15715271294117644|\n",
      "|0.35747068|(1,[0],[0.05])|0.15715271294117644|\n",
      "|0.16675166|(1,[0],[0.06])|0.15715271294117644|\n",
      "|0.17491076|(1,[0],[0.07])|0.15715271294117644|\n",
      "| 0.0418154|(1,[0],[0.08])|0.15715271294117644|\n",
      "|0.04793473|(1,[0],[0.09])|0.15715271294117644|\n",
      "|0.03926568| (1,[0],[0.1])|0.15715271294117644|\n",
      "|0.12952575|(1,[0],[0.11])|0.15715271294117644|\n",
      "|       0.0|(1,[0],[0.12])|0.15715271294117644|\n",
      "|0.01376849|(1,[0],[0.13])|0.15715271294117644|\n",
      "|0.13105558|(1,[0],[0.14])|0.15715271294117644|\n",
      "|0.08873024|(1,[0],[0.15])|0.15715271294117644|\n",
      "|0.12595614|(1,[0],[0.16])|0.15715271294117644|\n",
      "|0.15247323|(1,[0],[0.17])|0.15715271294117644|\n",
      "|0.25956145|(1,[0],[0.18])|        0.189138196|\n",
      "|0.20040796|(1,[0],[0.19])|        0.189138196|\n",
      "|0.19581846| (1,[0],[0.2])|        0.189138196|\n",
      "+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"file:///Users/kaustuv/spark-3.1.1/data/mllib/sample_isotonic_regression_libsvm_data.txt\")\n",
    "\n",
    "# Trains an isotonic regression model.\n",
    "model = IsotonicRegression().fit(dataset)\n",
    "print(\"Boundaries in increasing order: %s\\n\" % str(model.boundaries))\n",
    "print(\"Predictions associated with the boundaries: %s\\n\" % str(model.predictions))\n",
    "\n",
    "# Makes predictions.\n",
    "model.transform(dataset).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Factorization machines regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## reqire versin >= 3.0\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.regression import FMRegressor\n",
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(spark_home +\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# # Scale features.\n",
    "# featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(data)\n",
    "\n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# # Train a FM model.\n",
    "# fm = FMRegressor(featuresCol=\"scaledFeatures\", stepSize=0.001)\n",
    "\n",
    "# # Create a Pipeline.\n",
    "# pipeline = Pipeline(stages=[featureScaler, fm])\n",
    "\n",
    "# # Train model.\n",
    "# model = pipeline.fit(trainingData)\n",
    "\n",
    "# # Make predictions.\n",
    "# predictions = model.transform(testData)\n",
    "\n",
    "# # Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# # Select (prediction, true label) and compute test error\n",
    "# evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# fmModel = model.stages[1]\n",
    "# print(\"Factors: \" + str(fmModel.factors))  # type: ignore\n",
    "# print(\"Linear: \" + str(fmModel.linear))  # type: ignore\n",
    "# print(\"Intercept: \" + str(fmModel.intercept))  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear methods\n",
    "### Streaming linear regression\n",
    "\n",
    "When data arrive in a streaming fashion, it is useful to fit regression models online, updating the parameters of the model as new data arrives. spark.mllib currently supports streaming linear regression using ordinary least squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "#sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD\n",
    "\n",
    "def parse(lp):\n",
    "    label = float(lp[lp.find('(') + 1: lp.find(',')])\n",
    "    vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))\n",
    "    return LabeledPoint(label, vec)\n",
    "\n",
    "trainingData = ssc.textFileStream(sys.argv[1]).map(parse).cache()\n",
    "testData = ssc.textFileStream(sys.argv[2]).map(parse)\n",
    "\n",
    "numFeatures = 3\n",
    "model = StreamingLinearRegressionWithSGD()\n",
    "model.setInitialWeights([0.0, 0.0, 0.0])\n",
    "\n",
    "model.trainOn(trainingData)\n",
    "print(model.predictOnValues(testData.map(lambda lp: (lp.label, lp.features))))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics - RDD-based API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "The following code snippets illustrate how to load a sample dataset, train a binary classification algorithm on the data, and evaluate the performance of the algorithm by several binary evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 1.0\n",
      "Area under ROC = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Several of the methods available in scala are currently missing from pyspark\n",
    "# Load training data in LIBSVM format\n",
    "data = MLUtils.loadLibSVMFile(sc,spark_home + \"data/mllib/sample_binary_classification_data.txt\")\n",
    "\n",
    "# Split data into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "The following code snippets illustrate how to load a sample dataset, train a multiclass classification algorithm on the data, and evaluate the performance of the algorithm by several multiclass classification evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 1.0\n",
      "Recall = 1.0\n",
      "F1 Score = 1.0\n",
      "Class 0.0 precision = 0.6818181818181818\n",
      "Class 0.0 recall = 0.9375\n",
      "Class 0.0 F1 Measure = 0.7894736842105263\n",
      "Class 1.0 precision = 1.0\n",
      "Class 1.0 recall = 1.0\n",
      "Class 1.0 F1 Measure = 1.0\n",
      "Class 2.0 precision = 0.9285714285714286\n",
      "Class 2.0 recall = 0.65\n",
      "Class 2.0 F1 Measure = 0.7647058823529412\n",
      "Weighted recall = 0.8461538461538461\n",
      "Weighted precision = 0.8746253746253746\n",
      "Weighted F(1) Score = 0.8447249345082163\n",
      "Weighted F(0.5) Score = 0.8585331672376207\n",
      "Weighted false positive rate = 0.0718482905982906\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Load training data in LIBSVM format\n",
    "data = MLUtils.loadLibSVMFile(sc, spark_home +\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# Split data into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "\n",
    "# Statistics by class\n",
    "labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.6428571428571429\n",
      "Precision = 0.6666666666666666\n",
      "F1 measure = 0.6380952380952382\n",
      "Accuracy = 0.5476190476190477\n",
      "Class 0.0 precision = 1.0\n",
      "Class 0.0 recall = 0.8\n",
      "Class 0.0 F1 Measure = 0.888888888888889\n",
      "Class 1.0 precision = 0.6666666666666666\n",
      "Class 1.0 recall = 0.6666666666666666\n",
      "Class 1.0 F1 Measure = 0.6666666666666666\n",
      "Class 2.0 precision = 0.5\n",
      "Class 2.0 recall = 0.5\n",
      "Class 2.0 F1 Measure = 0.5\n",
      "Micro precision = 0.7272727272727273\n",
      "Micro recall = 0.6666666666666666\n",
      "Micro F1 measure = 0.6956521739130435\n",
      "Hamming loss = 0.3333333333333333\n",
      "Subset accuracy = 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "\n",
    "scoreAndLabels = sc.parallelize([\n",
    "    ([0.0, 1.0], [0.0, 2.0]),\n",
    "    ([0.0, 2.0], [0.0, 1.0]),\n",
    "    ([], [0.0]),\n",
    "    ([2.0], [2.0]),\n",
    "    ([2.0, 0.0], [2.0, 0.0]),\n",
    "    ([0.0, 1.0, 2.0], [0.0, 1.0]),\n",
    "    ([1.0], [1.0, 2.0])])\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MultilabelMetrics(scoreAndLabels)\n",
    "\n",
    "# Summary stats\n",
    "print(\"Recall = %s\" % metrics.recall())\n",
    "print(\"Precision = %s\" % metrics.precision())\n",
    "print(\"F1 measure = %s\" % metrics.f1Measure())\n",
    "print(\"Accuracy = %s\" % metrics.accuracy)\n",
    "\n",
    "# Individual label stats\n",
    "labels = scoreAndLabels.flatMap(lambda x: x[1]).distinct().collect()\n",
    "for label in labels:\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.f1Measure(label)))\n",
    "\n",
    "# Micro stats\n",
    "print(\"Micro precision = %s\" % metrics.microPrecision)\n",
    "print(\"Micro recall = %s\" % metrics.microRecall)\n",
    "print(\"Micro F1 measure = %s\" % metrics.microF1Measure)\n",
    "\n",
    "# Hamming loss\n",
    "print(\"Hamming loss = %s\" % metrics.hammingLoss)\n",
    "\n",
    "# Subset accuracy\n",
    "print(\"Subset accuracy = %s\" % metrics.subsetAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.23083564973126544\n",
      "R-squared = 0.9621738887042595\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Read in the ratings data\n",
    "lines = sc.textFile(spark_home +\"data/mllib/sample_movielens_data.txt\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(\"::\")\n",
    "    return Rating(int(fields[0]), int(fields[1]), float(fields[2]) - 2.5)\n",
    "ratings = lines.map(lambda r: parseLine(r))\n",
    "\n",
    "# Train a model on to predict user-product ratings\n",
    "model = ALS.train(ratings, 10, 10, 0.01)\n",
    "\n",
    "# Get predicted ratings on all existing user-product pairs\n",
    "testData = ratings.map(lambda p: (p.user, p.product))\n",
    "predictions = model.predictAll(testData).map(lambda r: ((r.user, r.product), r.rating))\n",
    "\n",
    "ratingsTuple = ratings.map(lambda r: ((r.user, r.product), r.rating))\n",
    "scoreAndLabels = predictions.join(ratingsTuple).map(lambda tup: tup[1])\n",
    "\n",
    "# Instantiate regression metrics to compare predicted and actual ratings\n",
    "metrics = RegressionMetrics(scoreAndLabels)\n",
    "\n",
    "# Root mean squared error\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
